pip install Flask flask-cors google-cloud-vision opencv-python-headless numpy
import os
import cv2
import numpy as np
import math
import io
import base64
from flask import Flask, request, jsonify
from flask_cors import CORS
from google.cloud import vision

# --- 1. Flask ì•± ë° Google Vision í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ---
app = Flask(__name__)
CORS(app) # ëª¨ë“  ë„ë©”ì¸ì—ì„œì˜ ìš”ì²­ì„ í—ˆìš© (í…ŒìŠ¤íŠ¸ìš©)

# [ì¤‘ìš”] Google Cloud ì„œë¹„ìŠ¤ ê³„ì • ì¸ì¦
# 1. Google Cloudì—ì„œ 'ì„œë¹„ìŠ¤ ê³„ì • í‚¤' (JSON íŒŒì¼)ë¥¼ ë‹¤ìš´ë¡œë“œ
# 2. ì´ app.py íŒŒì¼ê³¼ ê°™ì€ í´ë”ì— 'service-account.json' ì´ë¦„ìœ¼ë¡œ ì €ì¥
os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "service-account.json" 

try:
    vision_client = vision.ImageAnnotatorClient()
    print("âœ… Vision AI í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
except Exception as e:
    print(f"âŒ Vision AI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
    print("   'service-account.json' íŒŒì¼ì´ ì˜¬ë°”ë¥¸ì§€ í™•ì¸í•˜ì„¸ìš”.")


# --- 2. ì¸ë„¤ì¼ì§€ì •_ì™„ë£Œ.pyì˜ í•µì‹¬ ë¡œì§ (ìˆ˜ì • 4: 'GPT' í”¼ë“œë°±) ---
# (Colabì´ ì•„ë‹Œ ì„œë²„ í™˜ê²½ì— ë§ê²Œ ì¼ë¶€ ìˆ˜ì •ë¨)

LIKELIHOOD_SCORE = {
    'UNKNOWN': 0, 'VERY_UNLIKELY': 0, 'UNLIKELY': 1,
    'POSSIBLE': 2, 'LIKELY': 4, 'VERY_LIKELY': 5
}

def analyze_frame_for_thumbnail(image_bytes):
    image = vision.Image(content=image_bytes)
    response = vision_client.face_detection(image=image)
    faces = response.face_annotations

    if not faces:
        return 0, "ì–¼êµ´ ì—†ìŒ", "N/A"

    best_face_score = -999
    best_mouth_info = "N/A"

    for face in faces:
        base_quality_score = 0
        if LIKELIHOOD_SCORE.get(face.blurred_likelihood, 0) < 3: base_quality_score += 50
        if LIKELIHOOD_SCORE.get(face.under_exposed_likelihood, 0) < 3: base_quality_score += 20
        if abs(face.roll_angle) < 20 and abs(face.pan_angle) < 20: base_quality_score += 30
        if face.detection_confidence > 0.7: base_quality_score += 20

        api_score_norm = LIKELIHOOD_SCORE.get(face.joy_likelihood, 0) / 5.0
        landmarks = {lm.type_: lm.position for lm in face.landmarks}
        
        required_lm = [
            vision.FaceAnnotation.Landmark.Type.UPPER_LIP,
            vision.FaceAnnotation.Landmark.Type.LOWER_LIP,
            vision.FaceAnnotation.Landmark.Type.MOUTH_CENTER,
            vision.FaceAnnotation.Landmark.Type.MOUTH_LEFT,
            vision.FaceAnnotation.Landmark.Type.MOUTH_RIGHT
        ]
        
        if not all(lm_type in landmarks for lm_type in required_lm):
            landmark_score_norm = 0.0
            mouth_info_str = f"Joy:{api_score_norm*5:.0f}, Landmark:FAIL"
        else:
            lip_distance = abs(landmarks[vision.FaceAnnotation.Landmark.Type.UPPER_LIP].y -
                               landmarks[vision.FaceAnnotation.Landmark.Type.LOWER_LIP].y)
            center_y = landmarks[vision.FaceAnnotation.Landmark.Type.MOUTH_CENTER].y
            left_y = landmarks[vision.FaceAnnotation.Landmark.Type.MOUTH_LEFT].y
            right_y = landmarks[vision.FaceAnnotation.Landmark.Type.MOUTH_RIGHT].y
            curvature = (center_y - left_y) + (center_y - right_y)
            curvature_norm = np.clip(curvature / 15.0, 0, 1)
            lip_norm = np.clip(lip_distance / 10.0, 0, 1)
            landmark_score_norm = (curvature_norm * 0.7) + (lip_norm * 0.3)
            mouth_info_str = f"Joy:{api_score_norm*5:.0f}, Pull:{curvature:.2f}, Open:{lip_distance:.2f}"

        emotion_norm = (api_score_norm * 0.8) + (landmark_score_norm * 0.2)
        emotion_score = emotion_norm * 300
        score = base_quality_score + emotion_score

        if score > best_face_score:
            best_face_score = score
            best_mouth_info = mouth_info_str

    return best_face_score, "ì–¼êµ´ ìˆìŒ", best_mouth_info

def extract_frames_by_interval(video_path, sec_per_frame=0.25):
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print(f"ì˜¤ë¥˜: ë¹„ë””ì˜¤ íŒŒì¼ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}")
        return []

    fps = cap.get(cv2.CAP_PROP_FPS)
    frame_interval = int(fps * sec_per_frame)
    if frame_interval == 0: frame_interval = 1

    frames_data = []
    frame_count = 0

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        if frame_count % frame_interval == 0:
            ret_enc, buffer = cv2.imencode('.jpg', frame, [cv2.IMWRITE_JPEG_QUALITY, 90])
            if ret_enc:
                current_time_sec = frame_count / fps
                frames_data.append({
                    'time_sec': current_time_sec,
                    'image_bytes': buffer.tobytes(),
                    'image_cv2': frame 
                })
        frame_count += 1
    cap.release()
    print(f"âœ… ì´ {len(frames_data)}ê°œì˜ í”„ë ˆì„ì´ ì¶”ì¶œë˜ì—ˆìŠµë‹ˆë‹¤.")
    return frames_data

def find_best_thumbnail(video_path):
    frames = extract_frames_by_interval(video_path, sec_per_frame=0.25)
    if not frames:
        return None

    print(f"\nGoogle Vision AIë¡œ ê° í”„ë ˆì„ ë¶„ì„ ì‹œì‘ (ì´ {len(frames)}ê°œ)...")
    scored_frames = []

    for i, frame_data in enumerate(frames):
        score, status, mouth_info_str = analyze_frame_for_thumbnail(frame_data['image_bytes'])
        frame_data['score'] = score
        frame_data['status'] = status
        frame_data['mouth'] = mouth_info_str
        scored_frames.append(frame_data)
        
        if i % 20 == 0: # 20 í”„ë ˆì„ë§ˆë‹¤ ë¡œê·¸ ì¶œë ¥
             print(f"  [ì‹œê°„: {frame_data['time_sec']:.2f}s] (ì ìˆ˜: {int(score)}) ({mouth_info_str})")

    scored_frames.sort(key=lambda x: x['score'], reverse=True)
    best_thumbnail = scored_frames[0]

    if best_thumbnail['score'] <= 0:
        print("ê²°ê³¼: ğŸŒ„ ìœ ì˜ë¯¸í•œ ì–¼êµ´ ì—†ìŒ. 50% ì§€ì  í”„ë ˆì„ ë°˜í™˜")
        best_thumbnail = frames[len(frames) // 2]
    else:
        print(f"ê²°ê³¼: ğŸ˜ƒ AIê°€ 'ë² ìŠ¤íŠ¸ ì¸ë„¤ì¼' ì„ ì •! (ì ìˆ˜: {int(best_thumbnail['score'])})")

    # [ìˆ˜ì •] ì´ë¯¸ì§€ë¥¼ Base64ë¡œ ì¸ì½”ë”©í•˜ì—¬ ë°˜í™˜
    ret, buffer = cv2.imencode('.jpg', best_thumbnail['image_cv2'])
    if not ret:
        return None
    
    img_base64 = base64.b64encode(buffer).decode('utf-8')
    
    return {
        "time_sec": best_thumbnail['time_sec'],
        "score": best_thumbnail['score'],
        "image_base64": img_base64
    }

# --- 3. Flask API ì—”ë“œí¬ì¸íŠ¸ ì •ì˜ ---
@app.route('/analyze', methods=['POST'])
def analyze_video():
    if 'video' not in request.files:
        return jsonify({"error": "No video file provided"}), 400

    video_file = request.files['video']
    
    # íŒŒì¼ì„ ì„ì‹œ ì €ì¥
    temp_path = "temp_video.mov" # (ë³´ì•ˆì„ ìœ„í•´ ì‹¤ì œë¡œëŠ” unique ì´ë¦„ ì‚¬ìš©)
    video_file.save(temp_path)

    try:
        # AI ë¶„ì„ ì‹¤í–‰
        result = find_best_thumbnail(temp_path)
        
        if result is None:
            return jsonify({"error": "Failed to analyze video"}), 500
        
        # ì„±ê³µ ì‹œ, ì¸ë„¤ì¼(Base64)ê³¼ ì •ë³´ ë°˜í™˜
        return jsonify({
            "message": "Analysis successful",
            "best_time_sec": result['time_sec'],
            "score": int(result['score']),
            "image_base64": result['image_base64']
        })

    except Exception as e:
        print(f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return jsonify({"error": str(e)}), 500
    
    finally:
        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
        if os.path.exists(temp_path):
            os.remove(temp_path)

if __name__ == '__main__':
    app.run(debug=True, host='0.0.0.0', port=5000)
