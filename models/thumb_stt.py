"""
ì›ƒëŠ” ì–¼êµ´ ì¸ë„¤ì¼ + ìš”ì•½/ì œëª© ìƒì„±
FaceMesh + Flash (ìµœì í™” ë²„ì „)
"""

import os
import cv2
import base64
import json
import numpy as np
from pydub import AudioSegment
from pydub.effects import speedup
from google.cloud import vision
import google.generativeai as genai
import mediapipe as mp



# ============================================================
# 1. FaceMesh ê¸°ë°˜ ì›ƒëŠ” ì–¼êµ´ í›„ë³´ ê²€ì¶œ
# ============================================================
os.environ["CUDA_VISIBLE_DEVICES"] = "-1"
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "3"
os.environ["MEDIAPIPE_DISABLE_GPU"] = "1"

mp_face_mesh = mp.solutions.face_mesh

mesh_detector = mp_face_mesh.FaceMesh(
    static_image_mode=True,
    max_num_faces=1,
    refine_landmarks=False,  # GPU ì‚¬ìš©ë˜ëŠ” ì˜µì…˜ â†’ ë°˜ë“œì‹œ False
    min_detection_confidence=0.5,
    min_tracking_confidence=0.5
)



UPPER_LIP = 13
LOWER_LIP = 14
LEFT_MOUTH = 61
RIGHT_MOUTH = 291


def is_smile_candidate(frame):
    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    result = mesh_detector.process(rgb)

    if not result.multi_face_landmarks:
        return False

    lm = result.multi_face_landmarks[0].landmark
    h, w, _ = frame.shape

    def pos(idx):
        return np.array([lm[idx].x * w, lm[idx].y * h])

    upper = pos(UPPER_LIP)
    lower = pos(LOWER_LIP)
    left = pos(LEFT_MOUTH)
    right = pos(RIGHT_MOUTH)

    lip_distance = np.linalg.norm(upper - lower)
    center = (upper + lower) / 2
    curvature = (center[1] - left[1]) + (center[1] - right[1])

    smile_score = curvature * 0.6 + lip_distance * 0.4

    return smile_score > 6


# ============================================================
# 2. Vision API Batch ë¶„ì„
# ============================================================
LIKELIHOOD_SCORE = {
    "UNKNOWN": 0,
    "VERY_UNLIKELY": 0,
    "UNLIKELY": 1,
    "POSSIBLE": 2,
    "LIKELY": 4,
    "VERY_LIKELY": 5,
}


def analyze_batch(frames):
    MAX_BATCH = 16
    all_results = []

    for i in range(0, len(frames), MAX_BATCH):
        chunk = frames[i:i + MAX_BATCH]

        requests = [
            vision.AnnotateImageRequest(
                image=vision.Image(content=f["image_bytes"]),
                features=[vision.Feature(type_=vision.Feature.Type.FACE_DETECTION)]
            )
            for f in chunk
        ]

        response = vision_client.batch_annotate_images(requests=requests)

        for frame, res in zip(chunk, response.responses):
            faces = res.face_annotations

            if not faces:
                frame["score"] = 0
                all_results.append(frame)
                continue

            total_score = 0
            for face in faces:
                blur_val = LIKELIHOOD_SCORE.get(face.blurred_likelihood.name, 0)
                under_val = LIKELIHOOD_SCORE.get(face.under_exposed_likelihood.name, 0)
                joy_val = LIKELIHOOD_SCORE.get(face.joy_likelihood.name, 0)

                base = 0
                if blur_val < 3: base += 40
                if under_val < 3: base += 20
                if abs(face.roll_angle) < 20 and abs(face.pan_angle) < 20:
                    base += 20

                joy_score = joy_val / 5.0 * 300
                total_score += base + joy_score

            frame["score"] = total_score
            all_results.append(frame)

    return all_results



# ============================================================
# 0. Blur ì²´í¬ í•¨ìˆ˜ (í”ë“¤ë¦° í”„ë ˆì„ ì™„ì „ ì œê±°)
# ============================================================
def is_blurry(frame, threshold=80):
    """
    Laplacian variance ê¸°ë°˜ í”ë“¤ë¦¼ ê°ì§€
    threshold â†‘ : ë” ì—„ê²© (80~120 ê¶Œì¥)
    """
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    val = cv2.Laplacian(gray, cv2.CV_64F).var()
    return val < threshold


# ============================================================
# 1. ì›ƒëŠ” ì–¼êµ´ í›„ë³´ + Blur ì œê±°
# ============================================================
def is_smile_candidate(frame):
    # ğŸ”¥ 1) Blur ë¨¼ì € ê²€ì‚¬
    if is_blurry(frame):
        return False

    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    result = mesh_detector.process(rgb)

    if not result.multi_face_landmarks:
        return False

    lm = result.multi_face_landmarks[0].landmark
    h, w, _ = frame.shape

    def pos(idx):
        return np.array([lm[idx].x * w, lm[idx].y * h])

    upper = pos(UPPER_LIP)
    lower = pos(LOWER_LIP)
    left = pos(LEFT_MOUTH)
    right = pos(RIGHT_MOUTH)

    lip_distance = np.linalg.norm(upper - lower)
    center = (upper + lower) / 2
    curvature = (center[1] - left[1]) + (center[1] - right[1])

    smile_score = curvature * 0.6 + lip_distance * 0.4

    # ğŸ”¥ 2) ì›ƒìŒ ì ìˆ˜ threshold ì•½ê°„ ìƒí–¥
    return smile_score > 8   # ê¸°ì¡´ 6 â†’ 8 (ì›ƒëŠ” ì–¼êµ´ë§Œ ë‚¨ê¹€)
# ============================================================
# 3. ì›ƒëŠ” ì–¼êµ´ í›„ë³´ í”„ë ˆì„ ì¶”ì¶œ
# ============================================================

def extract_candidate_frames(video_path, sec_interval=0.35):
    cap = cv2.VideoCapture(video_path)
    fps = cap.get(cv2.CAP_PROP_FPS) or 30.0
    step = max(int(fps * sec_interval), 1)

    frames = []
    frame_idx = 0
    total = 0

    while True:
        ret, frame = cap.read()
        if not ret:
            break
        total += 1

        if frame_idx % step == 0:
            if is_smile_candidate(frame):
                ok, buffer = cv2.imencode(".jpg", frame)
                if ok:
                    frames.append({
                        "time_sec": frame_idx / fps,
                        "image_cv2": frame,
                        "image_bytes": buffer.tobytes(),
                    })

        frame_idx += 1

    cap.release()
    print(f"âš¡ ì „ì²´ {total}í”„ë ˆì„ â†’ í›„ë³´ {len(frames)}ê°œ")
    return frames


# ============================================================
# 4. ìµœì¢… ì¸ë„¤ì¼ ì„ íƒ
# ============================================================
def find_best_thumbnail(video_path):
    candidates = extract_candidate_frames(video_path)

    if len(candidates) == 0:
        return None

    if len(candidates) > 12:
        candidates = candidates[:12]

    scored = analyze_batch(candidates)
    scored.sort(key=lambda x: x["score"], reverse=True)

    best = scored[0]
    ok, buffer = cv2.imencode(".jpg", best["image_cv2"])
    img_base64 = base64.b64encode(buffer).decode("utf-8")

    print(f"ğŸ‰ ìµœì¢… ì¸ë„¤ì¼ (score={best['score']:.1f}, time={best['time_sec']:.2f}s)")

    return {
        "time_sec": best["time_sec"],
        "score": best["score"],
        "image_base64": img_base64,
    }


# ============================================================
# 5. ì˜¤ë””ì˜¤ ì¶”ì¶œ â†’ 1.2x â†’ Gemini (ë¬´ìŒ ì œê±° ì—†ìŒ)
# ============================================================
# ============================================================
# 5. ì˜¤ë””ì˜¤ ì¶”ì¶œ â†’ 1.2x â†’ Gemini (ë¬´ìŒ ì œê±° ì—†ìŒ)
# ============================================================
from pydub import AudioSegment
from pydub.effects import speedup

def extract_audio(video_path, audio_path=None):
    try:
        # ğŸ”¥ audio_pathë¥¼ ëª…ì‹œí•˜ì§€ ì•Šìœ¼ë©´, ì›ë³¸ ê²½ë¡œ ê¸°ë°˜ìœ¼ë¡œ ìë™ ë¶€ì—¬
        if audio_path is None:
            audio_path = f"{video_path}.audio.mp3"

        # ğŸ”¥ format ì¸ì ì œê±° â†’ ffmpegê°€ ì»¨í…Œì´ë„ˆ(webm/mp4) ìë™ ì¸ì‹
        audio = AudioSegment.from_file(video_path)

        # í•„ìš”í•˜ë©´ ë¬´ìŒ ì œê±° ë‹¤ì‹œ ë¶™ì¼ ìˆ˜ ìˆìŒ
        # audio = remove_silence(audio)

        # 1.2x ì†ë„ ì¦ê°€
        audio = speedup(audio, playback_speed=1.2, chunk_size=60, crossfade=40)

        audio.export(audio_path, format="mp3")
        return audio_path

    except Exception as e:
        raise RuntimeError(f"Audio extraction failed: {e}")


def analyze_video_content(video_path, api_key):
    if not api_key:
        raise ValueError("ìœ íš¨í•œ Google API Key í•„ìš”")

    genai.configure(api_key=api_key)

    audio_file_path = extract_audio(video_path)

    try:
        with open(audio_file_path, "rb") as f:
            audio_bytes = f.read()

        model = genai.GenerativeModel("gemini-2.5-flash")


        prompt = """
        ì´ ì˜¤ë””ì˜¤ ë‚´ìš©ì„ í•œêµ­ì–´ë¡œ í•œ ë¬¸ì¥ ìš”ì•½í•˜ë˜,ì£¼ë³€ ì†ŒìŒë³´ë‹¤ ë°œí™” ë‚´ìš©ì„ ìš°ì„ ìœ¼ë¡œ ë‚´ìš©ì„ ìš”ì•½í•˜ì„¸ìš”.
        ì˜ìƒì˜ ì£¼ì œë¥¼ ë°˜ì˜í•œ ê°„ê²°í•œ ì œëª©ì„ ìƒì„±í•˜ì„¸ìš”. 
        ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œ:
        {
          "summary": "...",
          "title": "..."
        }
        """

        response = model.generate_content(
            [
                {"mime_type": "audio/mpeg", "data": audio_bytes},
                prompt
            ]
        )

        clean = response.text.strip().lstrip("```json").rstrip("```").strip()
        result = json.loads(clean)

        return {
            "summary": result.get("summary", ""),
            "title": result.get("title", "")
        }

    except Exception as e:
        raise RuntimeError(f"Gemini ë¶„ì„ ì˜¤ë¥˜: {e}")

    finally:
        if os.path.exists(audio_file_path):
            os.remove(audio_file_path)
