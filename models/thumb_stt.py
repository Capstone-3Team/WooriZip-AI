"""
ì›ƒëŠ” ì–¼êµ´ ì¸ë„¤ì¼ + ìš”ì•½/ì œëª© ìƒì„±
FaceMesh + Flash (ìµœì í™” ë²„ì „)
"""

import os
import cv2
import base64
import json
import numpy as np
from pydub import AudioSegment
from pydub.effects import speedup
from google.cloud import vision
import google.generativeai as genai
import mediapipe as mp

# ============================================================
# 0. Vision API ì´ˆê¸°í™”
# ============================================================
if not os.environ.get("GOOGLE_APPLICATION_CREDENTIALS"):
    os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "service-account.json"

vision_client = vision.ImageAnnotatorClient()


# ============================================================
# (OPTIONAL) â€” ë¬´ìŒ ì œê±° í•¨ìˆ˜ (ë” ì´ìƒ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
# ============================================================
def remove_silence(audio: AudioSegment,
                   min_silence_len=800,
                   silence_thresh=-45):
    """
    (ì§€ê¸ˆì€ ì‚¬ìš© ì•ˆ í•¨)
    """
    return audio


# ============================================================
# 1. FaceMesh ê¸°ë°˜ ì›ƒëŠ” ì–¼êµ´ í›„ë³´ ê²€ì¶œ
# ============================================================
mp_face_mesh = mp.solutions.face_mesh
mesh_detector = mp_face_mesh.FaceMesh(
    max_num_faces=1,
    refine_landmarks=True,
    min_detection_confidence=0.5,
    min_tracking_confidence=0.5
)

UPPER_LIP = 13
LOWER_LIP = 14
LEFT_MOUTH = 61
RIGHT_MOUTH = 291


def is_smile_candidate(frame):
    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    result = mesh_detector.process(rgb)

    if not result.multi_face_landmarks:
        return False

    lm = result.multi_face_landmarks[0].landmark
    h, w, _ = frame.shape

    def pos(idx):
        return np.array([lm[idx].x * w, lm[idx].y * h])

    upper = pos(UPPER_LIP)
    lower = pos(LOWER_LIP)
    left = pos(LEFT_MOUTH)
    right = pos(RIGHT_MOUTH)

    lip_distance = np.linalg.norm(upper - lower)
    center = (upper + lower) / 2
    curvature = (center[1] - left[1]) + (center[1] - right[1])

    smile_score = curvature * 0.6 + lip_distance * 0.4

    return smile_score > 6


# ============================================================
# 2. Vision API Batch ë¶„ì„
# ============================================================
LIKELIHOOD_SCORE = {
    "UNKNOWN": 0,
    "VERY_UNLIKELY": 0,
    "UNLIKELY": 1,
    "POSSIBLE": 2,
    "LIKELY": 4,
    "VERY_LIKELY": 5,
}


def analyze_batch(frames):
    MAX_BATCH = 16
    all_results = []

    for i in range(0, len(frames), MAX_BATCH):
        chunk = frames[i:i + MAX_BATCH]

        requests = [
            vision.AnnotateImageRequest(
                image=vision.Image(content=f["image_bytes"]),
                features=[vision.Feature(type_=vision.Feature.Type.FACE_DETECTION)]
            )
            for f in chunk
        ]

        response = vision_client.batch_annotate_images(requests=requests)

        for frame, res in zip(chunk, response.responses):
            faces = res.face_annotations

            if not faces:
                frame["score"] = 0
                all_results.append(frame)
                continue

            total_score = 0
            for face in faces:
                blur_val = LIKELIHOOD_SCORE.get(face.blurred_likelihood.name, 0)
                under_val = LIKELIHOOD_SCORE.get(face.under_exposed_likelihood.name, 0)
                joy_val = LIKELIHOOD_SCORE.get(face.joy_likelihood.name, 0)

                base = 0
                if blur_val < 3: base += 40
                if under_val < 3: base += 20
                if abs(face.roll_angle) < 20 and abs(face.pan_angle) < 20:
                    base += 20

                joy_score = joy_val / 5.0 * 300
                total_score += base + joy_score

            frame["score"] = total_score
            all_results.append(frame)

    return all_results


# ============================================================
# 3. ì›ƒëŠ” ì–¼êµ´ í›„ë³´ í”„ë ˆì„ ì¶”ì¶œ
# ============================================================
def extract_candidate_frames(video_path, sec_interval=0.35):
    cap = cv2.VideoCapture(video_path)
    fps = cap.get(cv2.CAP_PROP_FPS) or 30.0
    step = max(int(fps * sec_interval), 1)

    frames = []
    frame_idx = 0
    total = 0

    while True:
        ret, frame = cap.read()
        if not ret:
            break
        total += 1

        if frame_idx % step == 0:
            if is_smile_candidate(frame):
                ok, buffer = cv2.imencode(".jpg", frame)
                if ok:
                    frames.append({
                        "time_sec": frame_idx / fps,
                        "image_cv2": frame,
                        "image_bytes": buffer.tobytes(),
                    })

        frame_idx += 1

    cap.release()
    print(f"âš¡ ì „ì²´ {total}í”„ë ˆì„ â†’ í›„ë³´ {len(frames)}ê°œ")
    return frames


# ============================================================
# 4. ìµœì¢… ì¸ë„¤ì¼ ì„ íƒ
# ============================================================
def find_best_thumbnail(video_path):
    candidates = extract_candidate_frames(video_path)

    if len(candidates) == 0:
        return None

    if len(candidates) > 12:
        candidates = candidates[:12]

    scored = analyze_batch(candidates)
    scored.sort(key=lambda x: x["score"], reverse=True)

    best = scored[0]
    ok, buffer = cv2.imencode(".jpg", best["image_cv2"])
    img_base64 = base64.b64encode(buffer).decode("utf-8")

    print(f"ğŸ‰ ìµœì¢… ì¸ë„¤ì¼ (score={best['score']:.1f}, time={best['time_sec']:.2f}s)")

    return {
        "time_sec": best["time_sec"],
        "score": best["score"],
        "image_base64": img_base64,
    }


# ============================================================
# 5. ì˜¤ë””ì˜¤ ì¶”ì¶œ â†’ 1.2x â†’ Gemini (ë¬´ìŒ ì œê±° ì—†ìŒ)
# ============================================================
def extract_audio(video_path, audio_path="temp_audio.mp3"):
    try:
        audio = AudioSegment.from_file(video_path)

        # âŒ ë¬´ìŒ ì œê±° ì œê±°ë¨
        # audio = remove_silence(audio)

        # ğŸ”¥ 1.2x ì†ë„ ì¦ê°€ (ì›í•œë‹¤ë©´ ì´ê²ƒë„ ëŒ ìˆ˜ ìˆìŒ)
        audio = speedup(audio, playback_speed=1.2, chunk_size=60, crossfade=40)

        audio.export(audio_path, format="mp3")
        return audio_path

    except Exception as e:
        raise RuntimeError(f"Audio extraction failed: {e}")


def analyze_video_content(video_path, api_key):
    if not api_key:
        raise ValueError("ìœ íš¨í•œ Google API Key í•„ìš”")

    genai.configure(api_key=api_key)

    audio_file_path = extract_audio(video_path)

    try:
        with open(audio_file_path, "rb") as f:
            audio_bytes = f.read()

        model = genai.GenerativeModel("models/gemini-2.5-flash")

        prompt = """
        ì´ ì˜¤ë””ì˜¤ ë‚´ìš©ì„ í•œêµ­ì–´ë¡œ í•œ ë¬¸ì¥ ìš”ì•½í•˜ë˜,ì£¼ë³€ ì†ŒìŒë³´ë‹¤ ë°œí™” ë‚´ìš©ì„ ìš°ì„ ìœ¼ë¡œ ë‚´ìš©ì„ ìš”ì•½í•˜ì„¸ìš”.
        ì˜ìƒì˜ ì£¼ì œë¥¼ ë°˜ì˜í•œ ê°„ê²°í•œ ì œëª©ì„ ìƒì„±í•˜ì„¸ìš”. 
        ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œ:
        {
          "summary": "...",
          "title": "..."
        }
        """

        response = model.generate_content(
            [
                {"mime_type": "audio/mpeg", "data": audio_bytes},
                prompt
            ]
        )

        clean = response.text.strip().lstrip("```json").rstrip("```").strip()
        result = json.loads(clean)

        return {
            "summary": result.get("summary", ""),
            "title": result.get("title", "")
        }

    except Exception as e:
        raise RuntimeError(f"Gemini ë¶„ì„ ì˜¤ë¥˜: {e}")

    finally:
        if os.path.exists(audio_file_path):
            os.remove(audio_file_path)
