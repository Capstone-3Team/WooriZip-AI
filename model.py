"""
ì›ƒëŠ” ì–¼êµ´ ì¸ë„¤ì¼ + ìš”ì•½/ì œëª© ìƒì„± í†µí•© AI ëª¨ë“ˆ
ì†ë„ ê°œì„  ë°˜ì˜: í”„ë ˆì„ìˆ˜ ê°ì†Œ, Vision API í˜¸ì¶œ ì¶•ì†Œ, STT ì˜¤ë””ì˜¤ ê¸¸ì´ ë‹¨ì¶•
"""

import os
import cv2
import base64
import json
import numpy as np
from pydub import AudioSegment, effects, silence
from google.cloud import vision
import google.generativeai as genai
import mediapipe as mp

# ============================================================
# 0. Vision API ì´ˆê¸°í™”
# ============================================================
if not os.environ.get("GOOGLE_APPLICATION_CREDENTIALS"):
    os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "service-account.json"

vision_client = vision.ImageAnnotatorClient()

# Mediapipe ì´ˆê¸°í™”
mp_face = mp.solutions.face_detection
mp_facedetector = mp_face.FaceDetection(model_selection=0, min_detection_confidence=0.45)

# Likelihood ë§¤í•‘
LIKELIHOOD_SCORE = {
    "UNKNOWN": 0,
    "VERY_UNLIKELY": 0,
    "UNLIKELY": 1,
    "POSSIBLE": 2,
    "LIKELY": 4,
    "VERY_LIKELY": 5,
}


# ============================================================
# 1. Mediapipe 1ì°¨ í•„í„°ë§ â€” ì›ƒìŒ í›„ë³´
# ============================================================
def is_smile_candidate(frame):
    results = mp_facedetector.process(frame)
    if not results.detections:
        return False

    det = results.detections[0]
    box = det.location_data.relative_bounding_box
    h, w, _ = frame.shape
    x1, y1 = int(box.xmin * w), int(box.ymin * h)
    x2, y2 = x1 + int(box.width * w), y1 + int(box.height * h)

    face_roi = frame[y1:y2, x1:x2]
    if face_roi.size == 0:
        return False

    roi_h = face_roi.shape[0]
    mouth_region = face_roi[int(roi_h * 0.55): int(roi_h * 0.85), :]
    if mouth_region.size == 0:
        return False

    gray = cv2.cvtColor(mouth_region, cv2.COLOR_BGR2GRAY)
    variance = gray.var()

    # ğŸš€ threshold ê°•í™” â†’ í›„ë³´ ìˆ˜ ê°ì†Œ
    return variance > 60


# ============================================================
# 2. Vision API â€” Batch ì–¼êµ´ ë¶„ì„ (16ì¥ì”©)
# ============================================================
def analyze_batch(frames):
    MAX_BATCH = 16
    all_results = []

    for i in range(0, len(frames), MAX_BATCH):
        chunk = frames[i:i + MAX_BATCH]

        requests = []
        for f in chunk:
            image = vision.Image(content=f["image_bytes"])
            req = vision.AnnotateImageRequest(
                image=image,
                features=[vision.Feature(type_=vision.Feature.Type.FACE_DETECTION)]
            )
            requests.append(req)

        response = vision_client.batch_annotate_images(requests=requests)

        for frame, res in zip(chunk, response.responses):
            faces = res.face_annotations

            if not faces:
                frame["score"] = 0
                all_results.append(frame)
                continue

            total_score = 0
            for face in faces:
                blur = LIKELIHOOD_SCORE.get(face.blurred_likelihood.name, 0)
                under = LIKELIHOOD_SCORE.get(face.under_exposed_likelihood.name, 0)
                joy = LIKELIHOOD_SCORE.get(face.joy_likelihood.name, 0)

                base_q = 0
                if blur < 3: base_q += 40
                if under < 3: base_q += 20
                if abs(face.roll_angle) < 20 and abs(face.pan_angle) < 20:
                    base_q += 20

                joy_score = joy / 5.0 * 300
                total_score += base_q + joy_score

            frame["score"] = total_score
            all_results.append(frame)

    return all_results


# ============================================================
# 3. í”„ë ˆì„ ì¶”ì¶œ + 1ì°¨ í•„í„°ë§
# ============================================================
def extract_candidate_frames(video_path, sec_interval=0.7):
    """0.7ì´ˆë§ˆë‹¤ ìƒ˜í”Œë§í•˜ì—¬ Mediapipeë¡œ í›„ë³´ ì¶”ë¦¼ â†’ ì†ë„ ë§¤ìš° ë¹¨ë¼ì§"""
    cap = cv2.VideoCapture(video_path)
    fps = cap.get(cv2.CAP_PROP_FPS) or 30.0
    step = max(int(fps * sec_interval), 1)

    frames = []
    frame_idx = 0
    total = 0

    while True:
        ret, frame = cap.read()
        if not ret:
            break
        total += 1

        if frame_idx % step == 0:
            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            if is_smile_candidate(rgb):
                ok, buf = cv2.imencode(".jpg", frame)
                if ok:
                    frames.append({
                        "time_sec": frame_idx / fps,
                        "image_bytes": buf.tobytes(),
                        "image_cv2": frame,
                    })

        frame_idx += 1

    cap.release()
    print(f"âš¡ ì „ì²´ {total}í”„ë ˆì„ â†’ í›„ë³´ {len(frames)}ê°œ")
    return frames


# ============================================================
# 4. ìµœì¢… ì¸ë„¤ì¼
# ============================================================
def find_best_thumbnail(video_path):
    candidates = extract_candidate_frames(video_path)

    if len(candidates) == 0:
        return None

    # ğŸš€ Vision API í˜¸ì¶œ ìˆ˜ ê°ì†Œ (30 â†’ 12)
    if len(candidates) > 12:
        candidates = candidates[:12]

    scored = analyze_batch(candidates)
    scored.sort(key=lambda x: x["score"], reverse=True)
    best = scored[0]

    ok, buf = cv2.imencode(".jpg", best["image_cv2"])
    img_b64 = base64.b64encode(buf).decode("utf-8")

    return {
        "time_sec": best["time_sec"],
        "score": best["score"],
        "image_base64": img_b64,
    }


# ============================================================
# 5. STT â€” ìš”ì•½ + ì œëª© (ì˜¤ë””ì˜¤ ê¸¸ì´ ë‹¨ì¶•)
# ============================================================
def extract_audio(video_path, audio_path="temp_audio.mp3"):
    """ë¬´ìŒ ì œê±° + ì†ë„ 1.15x â†’ ì˜¤ë””ì˜¤ ê¸¸ì´ ìì²´ë¥¼ ë‹¨ì¶•"""
    try:
        audio = AudioSegment.from_file(video_path)

        # ë¬´ìŒ ì œê±°
        audio = silence.strip_silence(
            audio,
            silence_thresh=-45,  # ê°ì§€ ë¯¼ê°ë„
            padding=200
        )

        # 1.15ë°° ì†ë„ (pitch ìœ ì§€)
        audio = effects.speedup(audio, playback_speed=1.15)

        audio.export(audio_path, format="mp3")
        return audio_path

    except Exception as e:
        raise RuntimeError(f"Audio extraction failed: {e}")


def analyze_video_content(video_path, api_key):
    if not api_key or api_key.strip() == "":
        raise ValueError("ìœ íš¨í•œ Google API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤.")

    genai.configure(api_key=api_key)

    audio_path = extract_audio(video_path)

    try:
        with open(audio_path, "rb") as f:
            audio_bytes = f.read()

        model = genai.GenerativeModel("models/gemini-2.5-flash")

        prompt = """
JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”.
{
  "summary": "...",
  "title": "..."
}
"""

        response = model.generate_content(
            [
                {"mime_type": "audio/mpeg", "data": audio_bytes},
                prompt
            ]
        )

        if not response.text:
            raise RuntimeError("Gemini ì‘ë‹µ ì—†ìŒ (response.text is None)")

        clean = (
            response.text
            .replace("```json", "")
            .replace("```", "")
            .strip()
        )

        try:
            data = json.loads(clean)
        except:
            print("âŒ Gemini JSON íŒŒì‹± ì‹¤íŒ¨ â€” ì‘ë‹µ ì›ë³¸:")
            print(response.text)
            raise RuntimeError("Gemini JSON ì˜¤ë¥˜")

        return {
            "summary": data.get("summary", ""),
            "title": data.get("title", "")
        }

    finally:
        if os.path.exists(audio_path):
            os.remove(audio_path)
