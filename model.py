# model.py
"""
ì›ƒëŠ” ì–¼êµ´ ì¸ë„¤ì¼ + ìš”ì•½/ì œëª© ìƒì„± í†µí•© AI ëª¨ë“ˆ

- find_best_thumbnail(video_path): Google Cloud Vision ê¸°ë°˜ ì›ƒëŠ” ì–¼êµ´ ì¸ë„¤ì¼ ì„ ì •
- analyze_video_content(video_path, api_key): Gemini 2.5 Flash ê¸°ë°˜ ë‚´ìš© ìš”ì•½ + ì œëª© ìƒì„±
"""

import os
import cv2
import base64
import json
import numpy as np
from pydub import AudioSegment
from google.cloud import vision
import google.generativeai as genai
import mediapipe as mp

# ============================================================
# 0. Vision API ì´ˆê¸°í™”
# ============================================================
if not os.environ.get("GOOGLE_APPLICATION_CREDENTIALS"):
    os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "service-account.json"

vision_client = vision.ImageAnnotatorClient()

# Mediapipe ì´ˆê¸°í™”
mp_face = mp.solutions.face_detection
mp_facedetector = mp_face.FaceDetection(model_selection=0, min_detection_confidence=0.45)

# Likelihood ë§¤í•‘ (ë¬¸ìì—´ ê¸°ì¤€)
LIKELIHOOD_SCORE = {
    "UNKNOWN": 0,
    "VERY_UNLIKELY": 0,
    "UNLIKELY": 1,
    "POSSIBLE": 2,
    "LIKELY": 4,
    "VERY_LIKELY": 5,
}


# ============================================================
# 1. Mediapipe 1ì°¨ í•„í„°ë§ (ë¹ ë¥¸ ì–¼êµ´/ì›ƒìŒ í›„ë³´ íƒì§€)
# ============================================================
def is_smile_candidate(frame):
    """
    Mediapipeë¡œ ë¹ ë¥´ê²Œ ì›ƒì„ ê°€ëŠ¥ì„± ìˆëŠ” í”„ë ˆì„ì¸ì§€ íŒë‹¨
    - ì–¼êµ´ì´ ìˆëŠ”ì§€
    - ì… ì£¼ë³€ ë³€í™”ëŸ‰(variance)ìœ¼ë¡œ ëŒ€ëµì ì¸ í‘œì • ë³€í™” ì²´í¬
    """
    results = mp_facedetector.process(frame)
    if not results.detections:
        return False

    det = results.detections[0]

    box = det.location_data.relative_bounding_box
    h, w, _ = frame.shape
    x1, y1 = int(box.xmin * w), int(box.ymin * h)
    x2, y2 = x1 + int(box.width * w), y1 + int(box.height * h)

    face_roi = frame[y1:y2, x1:x2]
    if face_roi.size == 0:
        return False

    roi_h = face_roi.shape[0]
    mouth_region = face_roi[int(roi_h * 0.55): int(roi_h * 0.85), :]

    if mouth_region.size == 0:
        return False

    gray = cv2.cvtColor(mouth_region, cv2.COLOR_BGR2GRAY)
    variance = gray.var()

    return variance > 40  # ê²½í—˜ì  threshold


# ============================================================
# 2. Vision API Batch ì–¼êµ´ ë¶„ì„ (ì •í™•í•œ ê°ì •/ì›ƒìŒ íŒë³„)
#    - ìš”ì²­ë‹¹ ìµœëŒ€ 16ì¥ ì œí•œ ë•Œë¬¸ì— chunk ì²˜ë¦¬
#    - Likelihood ENUM â†’ .name ìœ¼ë¡œ ë³€í™˜í•´ì„œ ì ìˆ˜ ë§¤í•‘
# ============================================================
def analyze_batch(frames):
    """
    frames: [{ "time_sec", "image_bytes", "image_cv2" }, ...]
    ê° frameì— "score" í•„ë“œë¥¼ ì¶”ê°€í•´ì„œ ë°˜í™˜
    """
    MAX_BATCH = 16
    all_results = []

    for i in range(0, len(frames), MAX_BATCH):
        chunk = frames[i:i + MAX_BATCH]

        requests = []
        for f in chunk:
            image = vision.Image(content=f["image_bytes"])
            req = vision.AnnotateImageRequest(
                image=image,
                features=[vision.Feature(type_=vision.Feature.Type.FACE_DETECTION)]
            )
            requests.append(req)

        response = vision_client.batch_annotate_images(requests=requests)

        for frame, res in zip(chunk, response.responses):
            faces = res.face_annotations

            if not faces:
                frame["score"] = 0
                all_results.append(frame)
                continue

            total_score = 0
            for face in faces:
                # ENUM â†’ ë¬¸ìì—´(.name) ë³€í™˜ í›„ ë§¤í•‘
                blur_val = LIKELIHOOD_SCORE.get(face.blurred_likelihood.name, 0)
                under_val = LIKELIHOOD_SCORE.get(face.under_exposed_likelihood.name, 0)
                joy_val = LIKELIHOOD_SCORE.get(face.joy_likelihood.name, 0)

                base_quality = 0
                if blur_val < 3:
                    base_quality += 40
                if under_val < 3:
                    base_quality += 20
                if abs(face.roll_angle) < 20 and abs(face.pan_angle) < 20:
                    base_quality += 20

                joy_score = joy_val / 5.0 * 300
                total_score += base_quality + joy_score

            frame["score"] = total_score
            all_results.append(frame)

    return all_results


# ============================================================
# 3. í”„ë ˆì„ ì¶”ì¶œ + 1ì°¨ í•„í„°ë§
# ============================================================
def extract_candidate_frames(video_path, sec_interval=0.25):
    """
    ëª¨ë“  í”„ë ˆì„ì„ ë³´ì§€ ì•Šê³ ,
    sec_interval ê°„ê²©ìœ¼ë¡œ ìƒ˜í”Œë§ + Mediapipeë¡œ ì›ƒëŠ” í›„ë³´ë§Œ ë‚¨ê¹€
    """
    cap = cv2.VideoCapture(video_path)
    fps = cap.get(cv2.CAP_PROP_FPS) or 30.0
    step = max(int(fps * sec_interval), 1)

    frames = []
    frame_idx = 0
    total = 0

    while True:
        ret, frame = cap.read()
        if not ret:
            break
        total += 1

        if frame_idx % step == 0:
            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            if is_smile_candidate(rgb):
                ok, buffer = cv2.imencode(".jpg", frame)
                if ok:
                    frames.append({
                        "time_sec": frame_idx / fps,
                        "image_bytes": buffer.tobytes(),
                        "image_cv2": frame,
                    })

        frame_idx += 1

    cap.release()

    if len(frames) == 0:
        print(f"ğŸ˜¢ ì „ì²´ í”„ë ˆì„ {total}ê°œ ì¤‘ ì›ƒëŠ” í›„ë³´ ì—†ìŒ")
    else:
        print(f"âš¡ ì „ì²´ í”„ë ˆì„ {total} â†’ í›„ë³´ {len(frames)}ê°œ")

    return frames


# ============================================================
# 4. ë©”ì¸: ìµœì¢… ì¸ë„¤ì¼ ì°¾ê¸°
# ============================================================
def find_best_thumbnail(video_path):
    candidates = extract_candidate_frames(video_path)

    if len(candidates) == 0:
        return None

    # ë¹„ìš© ì ˆì•½ìš© ìƒí•œì„ 
    if len(candidates) > 30:
        candidates = candidates[:30]

    scored = analyze_batch(candidates)
    scored.sort(key=lambda x: x["score"], reverse=True)
    best = scored[0]

    print(f"ğŸ‰ ìµœì¢… ì¸ë„¤ì¼ (score={best['score']:.1f}, time={best['time_sec']:.2f}s)")

    ok, buffer = cv2.imencode(".jpg", best["image_cv2"])
    img_base64 = base64.b64encode(buffer).decode("utf-8")

    return {
        "time_sec": best["time_sec"],
        "score": best["score"],
        "image_base64": img_base64,
    }


# ============================================================
# 5. ë¹„ë””ì˜¤ â†’ ì˜¤ë””ì˜¤ ì¶”ì¶œ
# ============================================================
def extract_audio(video_path, audio_path="temp_audio.mp3"):
    """
    ë¹„ë””ì˜¤ íŒŒì¼ì—ì„œ ì˜¤ë””ì˜¤ë¥¼ ì¶”ì¶œí•˜ì—¬ mp3ë¡œ ì €ì¥
    """
    try:
        video = AudioSegment.from_file(video_path)
        video.export(audio_path, format="mp3")
        return audio_path
    except Exception as e:
        raise RuntimeError(f"Audio extraction failed: {e}")


# ============================================================
# 6. Gemini 2.5 Flash: ìš”ì•½ + ì œëª© ìƒì„±
# ============================================================
def analyze_video_content(video_path, api_key):
    """
    ë¹„ë””ì˜¤ â†’ ì˜¤ë””ì˜¤ ì¶”ì¶œ â†’ Gemini Flashë¡œ ìš”ì•½ + ì œëª© ìƒì„±
    transcript(ì „ì²´ STT)ëŠ” í¬í•¨í•˜ì§€ ì•Šê³  summary/titleë§Œ ë°˜í™˜
    """
    if api_key is None or api_key.strip() == "":
        raise ValueError("ìœ íš¨í•œ Google API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤.")

    genai.configure(api_key=api_key)

    audio_file_path = extract_audio(video_path)

    try:
        with open(audio_file_path, "rb") as f:
            audio_bytes = f.read()

        model = genai.GenerativeModel("models/gemini-2.5-flash")

        prompt = """
        ì´ ì˜¤ë””ì˜¤ ë‚´ìš©ì„ í•œêµ­ì–´ë¡œ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•˜ê³ ,
        ì˜ìƒì˜ ì£¼ì œë¥¼ ë°˜ì˜í•œ ê°„ê²°í•œ ì œëª©ì„ ìƒì„±í•˜ì„¸ìš”.

        ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”:
        {
          "summary": "...",
          "title": "..."
        }
        """

        response = model.generate_content(
            [
                {"mime_type": "audio/mpeg", "data": audio_bytes},
                prompt
            ]
        )

        text = response.text.strip()
        clean = text.lstrip("```json").lstrip("```").rstrip("```").strip()
        data = json.loads(clean)

        summary = data.get("summary", "")
        title = data.get("title", "")

        return {
            "summary": summary,
            "title": title
        }

    except Exception as e:
        raise RuntimeError(f"Gemini ë¶„ì„ ì˜¤ë¥˜: {e}")

    finally:
        if os.path.exists(audio_file_path):
            os.remove(audio_file_path)
