"""
ì›ƒëŠ” ì–¼êµ´ ì¸ë„¤ì¼ + ìš”ì•½/ì œëª© ìƒì„± í†µí•© AI ëª¨ë“ˆ
"""

import os
import cv2
import base64
import json
import numpy as np
from pydub import AudioSegment, effects, silence
from google.cloud import vision
import google.generativeai as genai
import mediapipe as mp


# ============================================================
# 0. Vision API ì´ˆê¸°í™”
# ============================================================
if not os.environ.get("GOOGLE_APPLICATION_CREDENTIALS"):
    os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "service-account.json"

vision_client = vision.ImageAnnotatorClient()

# Mediapipe ì´ˆê¸°í™”
mp_face = mp.solutions.face_detection
mp_facedetector = mp_face.FaceDetection(model_selection=0, min_detection_confidence=0.45)


# Likelihood ë§¤í•‘ (ENUM â†’ ë¬¸ìì—´ â†’ ì ìˆ˜)
LIKELIHOOD_SCORE = {
    "UNKNOWN": 0,
    "VERY_UNLIKELY": 0,
    "UNLIKELY": 1,
    "POSSIBLE": 2,
    "LIKELY": 4,
    "VERY_LIKELY": 5,
}


# ============================================================
# 1. Mediapipe 1ì°¨ í•„í„°ë§ (ë¹ ë¥¸ ì›ƒìŒ í›„ë³´ ì¶”ì¶œ)
# ============================================================
def is_smile_candidate(frame):
    results = mp_facedetector.process(frame)
    if not results.detections:
        return False

    det = results.detections[0]
    box = det.location_data.relative_bounding_box
    h, w, _ = frame.shape

    x1, y1 = int(box.xmin * w), int(box.ymin * h)
    x2, y2 = x1 + int(box.width * w), y1 + int(box.height * h)

    face_roi = frame[y1:y2, x1:x2]
    if face_roi.size == 0:
        return False

    roi_h = face_roi.shape[0]
    mouth_region = face_roi[int(roi_h * 0.55): int(roi_h * 0.85), :]
    if mouth_region.size == 0:
        return False

    gray = cv2.cvtColor(mouth_region, cv2.COLOR_BGR2GRAY)
    variance = gray.var()

    return variance > 40  # í‘œì • ë³€í™” ê¸°ì¤€


# ============================================================
# 2. Vision API Batch ì–¼êµ´ ë¶„ì„ (ë¹ ë¥´ê³  ì •í™•)
# ============================================================
def analyze_batch(frames):
    MAX_BATCH = 16
    all_results = []

    for i in range(0, len(frames), MAX_BATCH):
        chunk = frames[i:i + MAX_BATCH]

        requests = [
            vision.AnnotateImageRequest(
                image=vision.Image(content=f["image_bytes"]),
                features=[vision.Feature(type_=vision.Feature.Type.FACE_DETECTION)]
            ) for f in chunk
        ]

        response = vision_client.batch_annotate_images(requests=requests)

        for frame, res in zip(chunk, response.responses):
            faces = res.face_annotations

            if not faces:
                frame["score"] = 0
                all_results.append(frame)
                continue

            total_score = 0

            for face in faces:

                blur_val = LIKELIHOOD_SCORE.get(face.blurred_likelihood.name, 0)
                under_val = LIKELIHOOD_SCORE.get(face.under_exposed_likelihood.name, 0)
                joy_val = LIKELIHOOD_SCORE.get(face.joy_likelihood.name, 0)

                base_quality = 0
                if blur_val < 3: base_quality += 40
                if under_val < 3: base_quality += 20
                if abs(face.roll_angle) < 20 and abs(face.pan_angle) < 20: base_quality += 20

                joy_score = (joy_val / 5.0) * 300
                total_score += base_quality + joy_score

            frame["score"] = total_score
            all_results.append(frame)

    return all_results


# ============================================================
# 3. í”„ë ˆì„ ì¶”ì¶œ + í›„ë³´ í•„í„°ë§(ì†ë„ ìµœì í™”: 0.33ì´ˆ ê°„ê²© ìƒ˜í”Œë§)
# ============================================================
def extract_candidate_frames(video_path, sec_interval=0.33):
    cap = cv2.VideoCapture(video_path)
    fps = cap.get(cv2.CAP_PROP_FPS) or 30.0
    step = max(int(fps * sec_interval), 1)

    frames = []
    frame_idx = 0
    total = 0

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        total += 1

        if frame_idx % step == 0:
            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            if is_smile_candidate(rgb):
                ok, buffer = cv2.imencode(".jpg", frame)
                if ok:
                    frames.append({
                        "time_sec": frame_idx / fps,
                        "image_bytes": buffer.tobytes(),
                        "image_cv2": frame
                    })

        frame_idx += 1

    cap.release()

    print(f"âš¡ ì „ì²´ í”„ë ˆì„ {total} â†’ í›„ë³´ {len(frames)}")

    return frames


# ============================================================
# 4. ë©”ì¸: ìµœì¢… ì¸ë„¤ì¼ ì°¾ê¸°
# ============================================================
def find_best_thumbnail(video_path):
    candidates = extract_candidate_frames(video_path)

    if len(candidates) == 0:
        return None

    # Vision API ë¹„ìš© + ì†ë„ ê°œì„  â†’ 24ì¥ ì´ìƒì´ë©´ ìë¥´ê¸°
    if len(candidates) > 24:
        candidates = candidates[:24]

    scored = analyze_batch(candidates)
    scored.sort(key=lambda x: x["score"], reverse=True)

    best = scored[0]

    ok, buffer = cv2.imencode(".jpg", best["image_cv2"])
    img_base64 = base64.b64encode(buffer).decode("utf-8")

    print(f"ğŸ‰ ìµœì¢… ì¸ë„¤ì¼ ì„ íƒ (score={best['score']:.1f}, time={best['time_sec']:.2f}s)")

    return {
        "time_sec": best["time_sec"],
        "score": best["score"],
        "image_base64": img_base64
    }


# ============================================================
# 5. STTìš© ì˜¤ë””ì˜¤ ì¶”ì¶œ (ë¬´ìŒ ì œê±° + 1.15x ì†ë„)
# ============================================================
def extract_audio(video_path, audio_path="temp_audio.mp3"):
    try:
        audio = AudioSegment.from_file(video_path)

        # --- ë¬´ìŒ ì œê±° ---
        silent_ranges = silence.detect_silence(
            audio,
            min_silence_len=700,
            silence_thresh=-45
        )

        if len(silent_ranges) > 0:
            non_silenced = AudioSegment.empty()
            prev_end = 0

            for start, end in silent_ranges:
                non_silenced += audio[prev_end:start]
                prev_end = end

            non_silenced += audio[prev_end:]
            audio = non_silenced

        # --- 1.15ë°° ì†ë„ ì¦ê°€ ---
        audio = effects.speedup(audio, playback_speed=1.15)

        audio.export(audio_path, format="mp3")
        return audio_path

    except Exception as e:
        raise RuntimeError(f"Audio extraction failed: {e}")


# ============================================================
# 6. STT ìš”ì•½ + ì œëª© ìƒì„± (Gemini 2.5 Flash)
# ============================================================
def analyze_video_content(video_path, api_key):
    if api_key is None or api_key.strip() == "":
        raise ValueError("ìœ íš¨í•œ Google API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤.")

    genai.configure(api_key=api_key)

    audio_file_path = extract_audio(video_path)

    try:
        with open(audio_file_path, "rb") as f:
            audio_bytes = f.read()

        model = genai.GenerativeModel("models/gemini-2.5-flash")

        prompt = """
        ì´ ì˜¤ë””ì˜¤ ë‚´ìš©ì„ í•œêµ­ì–´ë¡œ í•œ ë¬¸ì¥ ìš”ì•½í•˜ê³ ,
        ì˜ìƒì˜ ì£¼ì œë¥¼ ë°˜ì˜í•œ ê°„ê²°í•œ ì œëª©ì„ ìƒì„±í•˜ì„¸ìš”.
        JSONìœ¼ë¡œë§Œ ëŒ€ë‹µ:
        {
          "summary": "...",
          "title": "..."
        }
        """

        response = model.generate_content(
            [
                {"mime_type": "audio/mpeg", "data": audio_bytes},
                prompt
            ]
        )

        clean = response.text.strip().lstrip("```json").rstrip("```").strip()
        data = json.loads(clean)

        return {
            "summary": data.get("summary", ""),
            "title": data.get("title", "")
        }

    except Exception as e:
        raise RuntimeError(f"Gemini ë¶„ì„ ì˜¤ë¥˜: {e}")

    finally:
        if os.path.exists(audio_file_path):
            os.remove(audio_file_path)
