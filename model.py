"""
ì›ƒëŠ” ì–¼êµ´ ì¸ë„¤ì¼ + ìš”ì•½/ì œëª© ìƒì„± (ì´ˆê³ ì† ìµœì í™” ë²„ì „)
"""

import os
import cv2
import base64
import json
import numpy as np
from pydub import AudioSegment
from pydub.effects import speedup
from google.cloud import vision
import google.generativeai as genai
import mediapipe as mp


# ============================================================
# 0. Vision API ì´ˆê¸°í™”
# ============================================================
if not os.environ.get("GOOGLE_APPLICATION_CREDENTIALS"):
    os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "service-account.json"

vision_client = vision.ImageAnnotatorClient()

# Mediapipe ì´ˆê¸°í™”
mp_face = mp.solutions.face_detection
mp_facedetector = mp_face.FaceDetection(model_selection=0, min_detection_confidence=0.50)

# Likelihood ë§¤í•‘
LIKELIHOOD_SCORE = {
    "UNKNOWN": 0,
    "VERY_UNLIKELY": 0,
    "UNLIKELY": 1,
    "POSSIBLE": 2,
    "LIKELY": 4,
    "VERY_LIKELY": 5,
}


# ============================================================
# 1. Mediapipe í›„ë³´ í•„í„°ë§ (ê°•í™” ë²„ì „)
# ============================================================
def is_smile_candidate(frame):
    results = mp_facedetector.process(frame)
    if not results.detections:
        return False

    det = results.detections[0]

    box = det.location_data.relative_bounding_box
    h, w, _ = frame.shape
    x1, y1 = int(box.xmin * w), int(box.ymin * h)
    x2, y2 = x1 + int(box.width * w), y1 + int(box.height * h)

    face_roi = frame[y1:y2, x1:x2]
    if face_roi.size == 0:
        return False

    roi_h = face_roi.shape[0]
    mouth_region = face_roi[int(roi_h * 0.55): int(roi_h * 0.85), :]
    if mouth_region.size == 0:
        return False

    gray = cv2.cvtColor(mouth_region, cv2.COLOR_BGR2GRAY)
    variance = gray.var()

    # â­ NEW: percentile ê¸°ë°˜ threshold (í•„í„° ë” ê°•í•¨)
    threshold = np.percentile(gray, 75)

    return variance > threshold


# ============================================================
# 2. Vision API Batch ë¶„ì„ (16ê°œ ë‹¨ìœ„)
# ============================================================
def analyze_batch(frames):
    MAX_BATCH = 16
    all_results = []

    for i in range(0, len(frames), MAX_BATCH):
        chunk = frames[i:i + MAX_BATCH]

        requests = [
            vision.AnnotateImageRequest(
                image=vision.Image(content=f["image_bytes"]),
                features=[vision.Feature(type_=vision.Feature.Type.FACE_DETECTION)]
            )
            for f in chunk
        ]

        response = vision_client.batch_annotate_images(requests=requests)

        for frame, res in zip(chunk, response.responses):
            faces = res.face_annotations
            if not faces:
                frame["score"] = 0
                all_results.append(frame)
                continue

            total_score = 0

            for face in faces:
                blur_val = LIKELIHOOD_SCORE.get(face.blurred_likelihood.name, 0)
                under_val = LIKELIHOOD_SCORE.get(face.under_exposed_likelihood.name, 0)
                joy_val = LIKELIHOOD_SCORE.get(face.joy_likelihood.name, 0)

                base_quality = 0
                if blur_val < 3:
                    base_quality += 40
                if under_val < 3:
                    base_quality += 20
                if abs(face.roll_angle) < 20 and abs(face.pan_angle) < 20:
                    base_quality += 20

                joy_score = joy_val / 5.0 * 300
                total_score += base_quality + joy_score

            frame["score"] = total_score
            all_results.append(frame)

    return all_results


# ============================================================
# 3. ë¹ ë¥¸ í›„ë³´ í”„ë ˆì„ ì¶”ì¶œ
# ============================================================
def extract_candidate_frames(video_path, sec_interval=0.35):
    cap = cv2.VideoCapture(video_path)
    fps = cap.get(cv2.CAP_PROP_FPS) or 30.0
    step = max(int(fps * sec_interval), 1)

    frames = []
    frame_idx = 0
    total = 0

    while True:
        ret, frame = cap.read()
        if not ret:
            break
        total += 1

        if frame_idx % step == 0:
            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            if is_smile_candidate(rgb):
                ok, buffer = cv2.imencode(".jpg", frame)
                if ok:
                    frames.append({
                        "time_sec": frame_idx / fps,
                        "image_bytes": buffer.tobytes(),
                        "image_cv2": frame,
                    })

        frame_idx += 1

    cap.release()

    print(f"âš¡ ì „ì²´ {total}í”„ë ˆì„ â†’ í›„ë³´ {len(frames)}ê°œ")
    return frames


# ============================================================
# 4. ìµœì¢… ì¸ë„¤ì¼ ì„ íƒ
# ============================================================
def find_best_thumbnail(video_path):
    candidates = extract_candidate_frames(video_path)

    if len(candidates) == 0:
        return None

    # ë¹„ìš©ì ˆê°: 16ì¥ê¹Œì§€ë§Œ Vision í˜¸ì¶œ
    if len(candidates) > 16:
        candidates = candidates[:16]

    scored = analyze_batch(candidates)
    scored.sort(key=lambda x: x["score"], reverse=True)
    best = scored[0]

    print(f"ğŸ‰ ìµœì¢… ì¸ë„¤ì¼ (score={best['score']:.1f}, time={best['time_sec']:.2f}s)")

    ok, buffer = cv2.imencode(".jpg", best["image_cv2"])
    img_base64 = base64.b64encode(buffer).decode("utf-8")

    return {
        "time_sec": best["time_sec"],
        "score": best["score"],
        "image_base64": img_base64,
    }


# ============================================================
# 5. ìš”ì•½ + ì œëª© ìƒì„± (Flash Lite + 1.2x ì˜¤ë””ì˜¤)
# ============================================================
def extract_audio(video_path, audio_path="temp_audio.mp3"):
    try:
        audio = AudioSegment.from_file(video_path)

        # â­ NEW: 1.2x ì†ë„ ì¦ê°€ (pitch ë³€í™” ê±°ì˜ ì—†ìŒ)
        audio = speedup(audio, playback_speed=1.2, chunk_size=60, crossfade=40)

        audio.export(audio_path, format="mp3")
        return audio_path
    except Exception as e:
        raise RuntimeError(f"Audio extraction failed: {e}")


def analyze_video_content(video_path, api_key):
    if not api_key:
        raise ValueError("ìœ íš¨í•œ Google API Key í•„ìš”")

    genai.configure(api_key=api_key)

    audio_file_path = extract_audio(video_path)

    try:
        with open(audio_file_path, "rb") as f:
            audio_bytes = f.read()

        model = genai.GenerativeModel("models/gemini-2.5-flash-lite")

        prompt = """
        ì´ ì˜¤ë””ì˜¤ ë‚´ìš©ì„ í•œêµ­ì–´ë¡œ í•œ ë¬¸ì¥ ìš”ì•½í•˜ê³ ,
        ì˜ìƒì˜ ì£¼ì œë¥¼ ë°˜ì˜í•œ ê°„ê²°í•œ ì œëª©ì„ ìƒì„±í•˜ì„¸ìš”.
        ë°˜ë“œì‹œ JSON:
        {
          "summary": "...",
          "title": "..."
        }
        """

        response = model.generate_content(
            [
                {"mime_type": "audio/mpeg", "data": audio_bytes},
                prompt
            ]
        )

        clean = response.text.strip().lstrip("```json").rstrip("```").strip()
        result = json.loads(clean)

        return {
            "summary": result.get("summary", ""),
            "title": result.get("title", "")
        }

    except Exception as e:
        raise RuntimeError(f"Gemini ë¶„ì„ ì˜¤ë¥˜: {e}")

    finally:
        if os.path.exists(audio_file_path):
            os.remove(audio_file_path)
