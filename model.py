import os
import cv2
import numpy as np
import math
import io
import base64
from google.cloud import vision

# --- 1. Google Vision í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ---
# [ì¤‘ìš”] ì´ íŒŒì¼ì„ ì‹¤í–‰í•˜ëŠ” ì„œë²„ í™˜ê²½ì— 'service-account.json'ì´ í•„ìš”í•©ë‹ˆë‹¤.
try:
    if not os.environ.get("GOOGLE_APPLICATION_CREDENTIALS"):
        os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "service-account.json"
        
    vision_client = vision.ImageAnnotatorClient()
    print("âœ… (model.py) Vision AI í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
except Exception as e:
    print(f"âŒ (model.py) Vision AI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
    print("   'service-account.json' íŒŒì¼ì´ ì˜¬ë°”ë¥¸ì§€ í™•ì¸í•˜ì„¸ìš”.")


# --- 2. AI ì ìˆ˜í™” ë¡œì§ ('GPT' í”¼ë“œë°± ë°˜ì˜) ---

LIKELIHOOD_SCORE = {
    'UNKNOWN': 0, 'VERY_UNLIKELY': 0, 'UNLIKELY': 1,
    'POSSIBLE': 2, 'LIKELY': 4, 'VERY_LIKELY': 5
}

def _analyze_frame_for_thumbnail(image_bytes):
    """
    (ë‚´ë¶€ í•¨ìˆ˜)
    ë‹¨ì¼ í”„ë ˆì„ ì´ë¯¸ì§€ë¥¼ Vision AIë¡œ ë¶„ì„í•˜ê³  ì ìˆ˜í™”í•©ë‹ˆë‹¤.
    """
    image = vision.Image(content=image_bytes)
    response = vision_client.face_detection(image=image)
    faces = response.face_annotations

    if not faces:
        return 0, "ì–¼êµ´ ì—†ìŒ", "N/A"

    best_face_score = -999
    best_mouth_info = "N/A"

    for face in faces:
        # 1. ê¸°ë³¸ í’ˆì§ˆ ì ìˆ˜ (0-120)
        base_quality_score = 0
        if LIKELIHOOD_SCORE.get(face.blurred_likelihood, 0) < 3: base_quality_score += 50
        if LIKELIHOOD_SCORE.get(face.under_exposed_likelihood, 0) < 3: base_quality_score += 20
        if abs(face.roll_angle) < 20 and abs(face.pan_angle) < 20: base_quality_score += 30
        if face.detection_confidence > 0.7: base_quality_score += 20

        # 2. ê°ì • ì ìˆ˜ (0-300)
        api_score_norm = LIKELIHOOD_SCORE.get(face.joy_likelihood, 0) / 5.0
        landmarks = {lm.type_: lm.position for lm in face.landmarks}
        
        required_lm = [
            vision.FaceAnnotation.Landmark.Type.UPPER_LIP,
            vision.FaceAnnotation.Landmark.Type.LOWER_LIP,
            vision.FaceAnnotation.Landmark.Type.MOUTH_CENTER,
            vision.FaceAnnotation.Landmark.Type.MOUTH_LEFT,
            vision.FaceAnnotation.Landmark.Type.MOUTH_RIGHT
        ]
        
        if not all(lm_type in landmarks for lm_type in required_lm):
            landmark_score_norm = 0.0
            mouth_info_str = f"Joy:{api_score_norm*5:.0f}, Landmark:FAIL"
        else:
            lip_distance = abs(landmarks[vision.FaceAnnotation.Landmark.Type.UPPER_LIP].y -
                               landmarks[vision.FaceAnnotation.Landmark.Type.LOWER_LIP].y)
            center_y = landmarks[vision.FaceAnnotation.Landmark.Type.MOUTH_CENTER].y
            left_y = landmarks[vision.FaceAnnotation.Landmark.Type.MOUTH_LEFT].y
            right_y = landmarks[vision.FaceAnnotation.Landmark.Type.MOUTH_RIGHT].y
            curvature = (center_y - left_y) + (center_y - right_y)
            curvature_norm = np.clip(curvature / 15.0, 0, 1)
            lip_norm = np.clip(lip_distance / 10.0, 0, 1)
            landmark_score_norm = (curvature_norm * 0.7) + (lip_norm * 0.3)
            mouth_info_str = f"Joy:{api_score_norm*5:.0f}, Pull:{curvature:.2f}, Open:{lip_distance:.2f}"

        emotion_norm = (api_score_norm * 0.8) + (landmark_score_norm * 0.2)
        emotion_score = emotion_norm * 300
        score = base_quality_score + emotion_score

        if score > best_face_score:
            best_face_score = score
            best_mouth_info = mouth_info_str

    return best_face_score, "ì–¼êµ´ ìˆìŒ", best_mouth_info

def _extract_frames_by_interval(video_path, sec_per_frame=0.25):
    """
    (ë‚´ë¶€ í•¨ìˆ˜)
    ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œì—ì„œ í”„ë ˆì„ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print(f"ì˜¤ë¥˜: ë¹„ë””ì˜¤ íŒŒì¼ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}")
        return []

    fps = cap.get(cv2.CAP_PROP_FPS)
    frame_interval = int(fps * sec_per_frame)
    if frame_interval == 0: frame_interval = 1

    frames_data = []
    frame_count = 0

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        if frame_count % frame_interval == 0:
            ret_enc, buffer = cv2.imencode('.jpg', frame, [cv2.IMWRITE_JPEG_QUALITY, 90])
            if ret_enc:
                current_time_sec = frame_count / fps
                frames_data.append({
                    'time_sec': current_time_sec,
                    'image_bytes': buffer.tobytes(),
                    'image_cv2': frame 
                })
        frame_count += 1
    cap.release()
    print(f"âœ… ì´ {len(frames_data)}ê°œì˜ í”„ë ˆì„ì´ ì¶”ì¶œë˜ì—ˆìŠµë‹ˆë‹¤.")
    return frames_data

# --- 3. app.pyê°€ í˜¸ì¶œí•  ë©”ì¸ í•¨ìˆ˜ ---

def find_best_thumbnail(video_path):
    """
    (ê³µê°œ í•¨ìˆ˜)
    ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œë¥¼ ë°›ì•„ AI ë¶„ì„ í›„, 
    ì¸ë„¤ì¼ ì •ë³´(Base64)ê°€ ë‹´ê¸´ ë”•ì…”ë„ˆë¦¬ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    frames = _extract_frames_by_interval(video_path, sec_per_frame=0.25)
    if not frames:
        return None

    print(f"\nGoogle Vision AIë¡œ ê° í”„ë ˆì„ ë¶„ì„ ì‹œì‘ (ì´ {len(frames)}ê°œ)...")
    scored_frames = []

    for i, frame_data in enumerate(frames):
        score, status, mouth_info_str = _analyze_frame_for_thumbnail(frame_data['image_bytes'])
        frame_data['score'] = score
        frame_data['status'] = status
        frame_data['mouth'] = mouth_info_str
        scored_frames.append(frame_data)
        
        if i % 20 == 0: # 20 í”„ë ˆì„ë§ˆë‹¤ ë¡œê·¸ ì¶œë ¥
             print(f"  [ì‹œê°„: {frame_data['time_sec']:.2f}s] (ì ìˆ˜: {int(score)}) ({mouth_info_str})")

    scored_frames.sort(key=lambda x: x['score'], reverse=True)
    best_thumbnail = scored_frames[0]

    if best_thumbnail['score'] <= 0:
        print("ê²°ê³¼: ğŸŒ„ ìœ ì˜ë¯¸í•œ ì–¼êµ´ ì—†ìŒ. 50% ì§€ì  í”„ë ˆì„ ë°˜í™˜")
        best_thumbnail = frames[len(frames) // 2]
    else:
        print(f"ê²°ê³¼: ğŸ˜ƒ AIê°€ 'ë² ìŠ¤íŠ¸ ì¸ë„¤ì¼' ì„ ì •! (ì ìˆ˜: {int(best_thumbnail['score'])})")

    # ì´ë¯¸ì§€ë¥¼ Base64ë¡œ ì¸ì½”ë”©í•˜ì—¬ ë°˜í™˜
    ret, buffer = cv2.imencode('.jpg', best_thumbnail['image_cv2'])
    if not ret:
        return None
    
    img_base64 = base64.b64encode(buffer).decode('utf-8')
    
    return {
        "time_sec": best_thumbnail['time_sec'],
        "score": best_thumbnail['score'],
        "image_base64": img_base64
    }
