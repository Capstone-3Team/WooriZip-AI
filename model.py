import os
import cv2
import numpy as np
import math
import io
import base64
from google.cloud import vision
from concurrent.futures import ThreadPoolExecutor, as_completed

# --- 1. Google Vision í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ---
try:
    if not os.environ.get("GOOGLE_APPLICATION_CREDENTIALS"):
        os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "service-account.json"
        
    vision_client = vision.ImageAnnotatorClient()
    print("âœ… (model.py) Vision AI í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
except Exception as e:
    print(f"âŒ (model.py) Vision AI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
    print("   'service-account.json' íŒŒì¼ì´ ì˜¬ë°”ë¥¸ì§€ í™•ì¸í•˜ì„¸ìš”.")


# --- 2. AI ì ìˆ˜í™” ë¡œì§ (ì—¬ëŸ¬ ì–¼êµ´ í•©ì‚° + GPT í”¼ë“œë°± ë°˜ì˜) ---

LIKELIHOOD_SCORE = {
    'UNKNOWN': 0, 'VERY_UNLIKELY': 0, 'UNLIKELY': 1,
    'POSSIBLE': 2, 'LIKELY': 4, 'VERY_LIKELY': 5
}


def _analyze_frame_for_thumbnail(image_bytes):
    """
    ì—¬ëŸ¬ ì–¼êµ´ì´ ë“±ì¥í•  ê²½ìš° ëª¨ë“  ì–¼êµ´ì˜ ì ìˆ˜ë¥¼ í•©ì‚°í•˜ì—¬
    'í•´ë‹¹ í”„ë ˆì„ ì „ì²´ ì ìˆ˜'ë¥¼ ë°˜í™˜í•œë‹¤.
    """
    image = vision.Image(content=image_bytes)
    response = vision_client.face_detection(image=image)
    faces = response.face_annotations

    if not faces:
        return 0, "ì–¼êµ´ ì—†ìŒ", "N/A"

    total_score = 0
    mouth_info_summary = []

    for face in faces:
        # -------- 1. ê¸°ë³¸ í’ˆì§ˆ ì ìˆ˜ --------
        base_quality_score = 0
        if LIKELIHOOD_SCORE.get(face.blurred_likelihood, 0) < 3: base_quality_score += 50
        if LIKELIHOOD_SCORE.get(face.under_exposed_likelihood, 0) < 3: base_quality_score += 20
        if abs(face.roll_angle) < 20 and abs(face.pan_angle) < 20: base_quality_score += 30
        if face.detection_confidence > 0.7: base_quality_score += 20

        # -------- 2. ê°ì • ì ìˆ˜ --------
        api_score_norm = LIKELIHOOD_SCORE.get(face.joy_likelihood, 0) / 5.0
        landmarks = {lm.type_: lm.position for lm in face.landmarks}

        required_lm = [
            vision.FaceAnnotation.Landmark.Type.UPPER_LIP,
            vision.FaceAnnotation.Landmark.Type.LOWER_LIP,
            vision.FaceAnnotation.Landmark.Type.MOUTH_CENTER,
            vision.FaceAnnotation.Landmark.Type.MOUTH_LEFT,
            vision.FaceAnnotation.Landmark.Type.MOUTH_RIGHT
        ]

        if not all(lm_type in landmarks for lm_type in required_lm):
            landmark_score_norm = 0.0
            mouth_info = f"Joy:{api_score_norm*5:.0f}, Landmark:FAIL"
        else:
            lip_distance = abs(
                landmarks[vision.FaceAnnotation.Landmark.Type.UPPER_LIP].y -
                landmarks[vision.FaceAnnotation.Landmark.Type.LOWER_LIP].y
            )

            center_y = landmarks[vision.FaceAnnotation.Landmark.Type.MOUTH_CENTER].y
            left_y = landmarks[vision.FaceAnnotation.Landmark.Type.MOUTH_LEFT].y
            right_y = landmarks[vision.FaceAnnotation.Landmark.Type.MOUTH_RIGHT].y

            curvature = (center_y - left_y) + (center_y - right_y)
            curvature_norm = np.clip(curvature / 15.0, 0, 1)
            lip_norm = np.clip(lip_distance / 10.0, 0, 1)

            landmark_score_norm = (curvature_norm * 0.7) + (lip_norm * 0.3)
            mouth_info = (
                f"Joy:{api_score_norm*5:.0f}, "
                f"Pull:{curvature:.2f}, Open:{lip_distance:.2f}"
            )

        # -------- 3. ê°ì • ì¢…í•© ì ìˆ˜ --------
        emotion_norm = (api_score_norm * 0.8) + (landmark_score_norm * 0.2)
        emotion_score = emotion_norm * 300

        face_score = base_quality_score + emotion_score
        total_score += face_score

        mouth_info_summary.append(mouth_info)

    return total_score, "ì—¬ëŸ¬ ì–¼êµ´", "; ".join(mouth_info_summary)


# ----------------------------------------------------
# 3. í”„ë ˆì„ ì¶”ì¶œ í•¨ìˆ˜
# ----------------------------------------------------

def _extract_frames_by_interval(video_path, sec_per_frame=0.25):
    """
    (ë‚´ë¶€ í•¨ìˆ˜)
    ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œì—ì„œ í”„ë ˆì„ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print(f"ì˜¤ë¥˜: ë¹„ë””ì˜¤ íŒŒì¼ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}")
        return []

    fps = cap.get(cv2.CAP_PROP_FPS)
    frame_interval = int(fps * sec_per_frame)
    if frame_interval == 0:
        frame_interval = 1

    frames_data = []
    frame_count = 0

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        if frame_count % frame_interval == 0:
            ret_enc, buffer = cv2.imencode('.jpg', frame, [cv2.IMWRITE_JPEG_QUALITY, 90])
            if ret_enc:
                current_time_sec = frame_count / fps
                frames_data.append({
                    'time_sec': current_time_sec,
                    'image_bytes': buffer.tobytes(),
                    'image_cv2': frame
                })

        frame_count += 1

    cap.release()
    print(f"âœ… ì´ {len(frames_data)}ê°œì˜ í”„ë ˆì„ì´ ì¶”ì¶œë˜ì—ˆìŠµë‹ˆë‹¤.")
    return frames_data


# ----------------------------------------------------
# 4. ë©”ì¸ í•¨ìˆ˜ â€” ë³‘ë ¬ ì²˜ë¦¬ + ì—¬ëŸ¬ ì–¼êµ´ í•©ì‚° + Vision API
# ----------------------------------------------------

def _process_frame(frame_data):
    """ ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ í”„ë ˆì„ ë¶„ì„ ë˜í¼ """
    try:
        score, status, mouth_info_str = _analyze_frame_for_thumbnail(frame_data['image_bytes'])
        frame_data['score'] = score
        frame_data['status'] = status
        frame_data['mouth'] = mouth_info_str
        return frame_data
    except Exception as e:
        print(f"Frame ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        return None


def find_best_thumbnail(video_path):
    """
    ë¹„ë””ì˜¤ íŒŒì¼ì„ ë¶„ì„í•˜ì—¬ ê°€ì¥ ì¢‹ì€ ì¸ë„¤ì¼ì„ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜ (ë³‘ë ¬ ì²˜ë¦¬ ê°œì„ )
    """
    frames = _extract_frames_by_interval(video_path, sec_per_frame=0.25)
    if not frames:
        return None

    print(f"\nGoogle Vision AI ë³‘ë ¬ ë¶„ì„ ì‹œì‘ (ì´ {len(frames)}ê°œ)...\n")

    scored_frames = []

    max_workers = min(10, len(frames))
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [executor.submit(_process_frame, f) for f in frames]

        for i, future in enumerate(as_completed(futures)):
            result = future.result()
            if result:
                scored_frames.append(result)

            if i % 10 == 0:
                print(f"  ì§„í–‰ë¥ : {i+1}/{len(frames)} í”„ë ˆì„ ì™„ë£Œ")

    scored_frames.sort(key=lambda x: x['score'], reverse=True)
    best_thumbnail = scored_frames[0]

    if best_thumbnail['score'] <= 0:
        print("ê²°ê³¼: ğŸŒ„ ìœ ì˜ë¯¸í•œ ì–¼êµ´ ì—†ìŒ. 50% ì§€ì  í”„ë ˆì„ ë°˜í™˜")
        best_thumbnail = frames[len(frames) // 2]
    else:
        print(f"ê²°ê³¼: ğŸ˜ƒ ë² ìŠ¤íŠ¸ ì¸ë„¤ì¼ ì„ ì •! (ì ìˆ˜: {int(best_thumbnail['score'])})")

    ret, buffer = cv2.imencode('.jpg', best_thumbnail['image_cv2'])
    if not ret:
        return None

    img_base64 = base64.b64encode(buffer).decode('utf-8')

    return {
        "time_sec": best_thumbnail['time_sec'],
        "score": best_thumbnail['score'],
        "image_base64": img_base64
    }
