# model.py
"""
ì›ƒëŠ” ì–¼êµ´ ì¸ë„¤ì¼ + STT/ìš”ì•½/ì œëª© ìƒì„± í†µí•© AI ëª¨ë“ˆ

- find_best_thumbnail(video_path): Google Cloud Vision ê¸°ë°˜ ì›ƒëŠ” ì–¼êµ´ ì¸ë„¤ì¼ ì„ ì •
- analyze_video_content(video_path, api_key): Gemini 2.5 Flash ê¸°ë°˜ STT + ìš”ì•½ + ì œëª© ìƒì„±
"""

import os
import json
import base64

import cv2
import numpy as np
from concurrent.futures import ThreadPoolExecutor, as_completed

from google.cloud import vision
import google.generativeai as genai
from pydub import AudioSegment


# =======================================
# 0. Google Vision í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
# =======================================

try:
    # í™˜ê²½ë³€ìˆ˜ì— ì„œë¹„ìŠ¤ ê³„ì • í‚¤ ê²½ë¡œê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ íŒŒì¼ëª… ì‚¬ìš©
    if not os.environ.get("GOOGLE_APPLICATION_CREDENTIALS"):
        os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "service-account.json"

    vision_client = vision.ImageAnnotatorClient()
    print("âœ… (model.py) Vision AI í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
except Exception as e:
    print(f"âŒ (model.py) Vision AI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
    print("   'service-account.json' íŒŒì¼ì´ ì˜¬ë°”ë¥¸ì§€ í™•ì¸í•˜ì„¸ìš”.")


# Vision APIì˜ likelihood ê°’ì„ ì •ëŸ‰í™”í•œ ìŠ¤ì½”ì–´ ë§µ
LIKELIHOOD_SCORE = {
    "UNKNOWN": 0,
    "VERY_UNLIKELY": 0,
    "UNLIKELY": 1,
    "POSSIBLE": 2,
    "LIKELY": 4,
    "VERY_LIKELY": 5,
}


# =======================================
# 1. ì›ƒëŠ” ì–¼êµ´ ì¸ë„¤ì¼ ë¶„ì„ ë¡œì§
# =======================================

def _analyze_frame_for_thumbnail(image_bytes):
    """
    í•œ í”„ë ˆì„(ì´ë¯¸ì§€)ì— ëŒ€í•´:
    - ì—¬ëŸ¬ ì–¼êµ´ì´ ë“±ì¥í•˜ë©´ ê° ì–¼êµ´ì˜ ì ìˆ˜ë¥¼ í•©ì‚°í•˜ì—¬
    - 'í”„ë ˆì„ ì „ì²´ ì ìˆ˜'ë¥¼ ë°˜í™˜
    """
    image = vision.Image(content=image_bytes)
    response = vision_client.face_detection(image=image)
    faces = response.face_annotations

    if not faces:
        return 0, "ì–¼êµ´ ì—†ìŒ", "N/A"

    total_score = 0
    mouth_info_summary = []

    for face in faces:
        # -------- 1. ê¸°ë³¸ í’ˆì§ˆ ì ìˆ˜ --------
        base_quality_score = 0

        # íë¦¼ ì •ë„ê°€ ì‹¬í•˜ì§€ ì•Šìœ¼ë©´ ê°€ì‚°ì 
        if LIKELIHOOD_SCORE.get(face.blurred_likelihood, 0) < 3:
            base_quality_score += 50
        # ë…¸ì¶œ ë¶€ì¡±ì´ ì‹¬í•˜ì§€ ì•Šìœ¼ë©´ ê°€ì‚°ì 
        if LIKELIHOOD_SCORE.get(face.under_exposed_likelihood, 0) < 3:
            base_quality_score += 20
        # ê¸°ìš¸ê¸°(roll/pan)ê°€ ì‹¬í•˜ì§€ ì•Šìœ¼ë©´ ê°€ì‚°ì 
        if abs(face.roll_angle) < 20 and abs(face.pan_angle) < 20:
            base_quality_score += 30
        # ì–¼êµ´ ê²€ì¶œ ì‹ ë¢°ë„ê°€ ë†’ì€ ê²½ìš° ê°€ì‚°ì 
        if face.detection_confidence > 0.7:
            base_quality_score += 20

        # -------- 2. ê°ì •(ì›ƒìŒ) ì ìˆ˜ --------
        api_score_norm = LIKELIHOOD_SCORE.get(face.joy_likelihood, 0) / 5.0

        landmarks = {lm.type_: lm.position for lm in face.landmarks}
        required_lm = [
            vision.FaceAnnotation.Landmark.Type.UPPER_LIP,
            vision.FaceAnnotation.Landmark.Type.LOWER_LIP,
            vision.FaceAnnotation.Landmark.Type.MOUTH_CENTER,
            vision.FaceAnnotation.Landmark.Type.MOUTH_LEFT,
            vision.FaceAnnotation.Landmark.Type.MOUTH_RIGHT,
        ]

        if not all(lm_type in landmarks for lm_type in required_lm):
            landmark_score_norm = 0.0
            mouth_info = f"Joy:{api_score_norm * 5:.0f}, Landmark:FAIL"
        else:
            # ì… ë²Œì–´ì§„ ì •ë„
            lip_distance = abs(
                landmarks[vision.FaceAnnotation.Landmark.Type.UPPER_LIP].y
                - landmarks[vision.FaceAnnotation.Landmark.Type.LOWER_LIP].y
            )

            # ì…ê¼¬ë¦¬ ì˜¬ë¼ê°„ ì •ë„ (ì¤‘ì•™ - ì¢Œ/ìš° ë†’ì´ ì°¨ì´)
            center_y = landmarks[vision.FaceAnnotation.Landmark.Type.MOUTH_CENTER].y
            left_y = landmarks[vision.FaceAnnotation.Landmark.Type.MOUTH_LEFT].y
            right_y = landmarks[vision.FaceAnnotation.Landmark.Type.MOUTH_RIGHT].y

            curvature = (center_y - left_y) + (center_y - right_y)

            curvature_norm = np.clip(curvature / 15.0, 0, 1)
            lip_norm = np.clip(lip_distance / 10.0, 0, 1)

            landmark_score_norm = curvature_norm * 0.7 + lip_norm * 0.3
            mouth_info = (
                f"Joy:{api_score_norm * 5:.0f}, "
                f"Pull:{curvature:.2f}, Open:{lip_distance:.2f}"
            )

        # -------- 3. ê°ì • ì¢…í•© ì ìˆ˜ --------
        emotion_norm = api_score_norm * 0.8 + landmark_score_norm * 0.2
        emotion_score = emotion_norm * 300

        face_score = base_quality_score + emotion_score
        total_score += face_score
        mouth_info_summary.append(mouth_info)

    return total_score, "ì—¬ëŸ¬ ì–¼êµ´", "; ".join(mouth_info_summary)


def _extract_frames_by_interval(video_path, sec_per_frame=0.25):
    """
    ë¹„ë””ì˜¤ íŒŒì¼ì—ì„œ ì¼ì • ê°„ê²©(sec_per_frame)ìœ¼ë¡œ í”„ë ˆì„ì„ ì¶”ì¶œ
    """
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print(f"ì˜¤ë¥˜: ë¹„ë””ì˜¤ íŒŒì¼ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}")
        return []

    fps = cap.get(cv2.CAP_PROP_FPS)
    frame_interval = int(fps * sec_per_frame)
    if frame_interval == 0:
        frame_interval = 1

    frames_data = []
    frame_count = 0

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        if frame_count % frame_interval == 0:
            ret_enc, buffer = cv2.imencode(".jpg", frame, [cv2.IMWRITE_JPEG_QUALITY, 90])
            if ret_enc:
                current_time_sec = frame_count / fps
                frames_data.append(
                    {
                        "time_sec": current_time_sec,
                        "image_bytes": buffer.tobytes(),
                        "image_cv2": frame,
                    }
                )

        frame_count += 1

    cap.release()
    print(f"âœ… ì´ {len(frames_data)}ê°œì˜ í”„ë ˆì„ì´ ì¶”ì¶œë˜ì—ˆìŠµë‹ˆë‹¤.")
    return frames_data


def _process_frame(frame_data):
    """
    ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ í”„ë ˆì„ ë¶„ì„ ë˜í¼
    """
    try:
        score, status, mouth_info_str = _analyze_frame_for_thumbnail(
            frame_data["image_bytes"]
        )
        frame_data["score"] = score
        frame_data["status"] = status
        frame_data["mouth"] = mouth_info_str
        return frame_data
    except Exception as e:
        print(f"Frame ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        return None


def find_best_thumbnail(video_path):
    """
    ë¹„ë””ì˜¤ íŒŒì¼ì„ ë¶„ì„í•˜ì—¬ ê°€ì¥ ì¢‹ì€ ì¸ë„¤ì¼ì„ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜ (ë³‘ë ¬ ì²˜ë¦¬)
    ë°˜í™˜ê°’:
    {
        "time_sec": <ì´ˆ ë‹¨ìœ„ í”„ë ˆì„ ìœ„ì¹˜>,
        "score": <ì¸ë„¤ì¼ ì ìˆ˜>,
        "image_base64": <JPG ì´ë¯¸ì§€ì˜ base64 ë¬¸ìì—´>
    }
    """
    frames = _extract_frames_by_interval(video_path, sec_per_frame=0.25)
    if not frames:
        return None

    print(f"\nGoogle Vision AI ë³‘ë ¬ ë¶„ì„ ì‹œì‘ (ì´ {len(frames)}ê°œ)...\n")

    scored_frames = []

    max_workers = min(10, len(frames))
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [executor.submit(_process_frame, f) for f in frames]

        for i, future in enumerate(as_completed(futures)):
            result = future.result()
            if result:
                scored_frames.append(result)

            if i % 10 == 0:
                print(f"  ì§„í–‰ë¥ : {i + 1}/{len(frames)} í”„ë ˆì„ ì™„ë£Œ")

    if not scored_frames:
        return None

    scored_frames.sort(key=lambda x: x["score"], reverse=True)
    best_thumbnail = scored_frames[0]

    # ì ìˆ˜ê°€ 0 ì´í•˜ì´ë©´, ì˜ë¯¸ ìˆëŠ” ì–¼êµ´ì´ ì—†ë‹¤ íŒë‹¨í•˜ê³  ì¤‘ê°„ í”„ë ˆì„ ë°˜í™˜
    if best_thumbnail["score"] <= 0:
        print("ê²°ê³¼: ğŸŒ„ ìœ ì˜ë¯¸í•œ ì–¼êµ´ ì—†ìŒ. 50% ì§€ì  í”„ë ˆì„ ë°˜í™˜")
        best_thumbnail = frames[len(frames) // 2]
    else:
        print(f"ê²°ê³¼: ğŸ˜ƒ ë² ìŠ¤íŠ¸ ì¸ë„¤ì¼ ì„ ì •! (ì ìˆ˜: {int(best_thumbnail['score'])})")

    ret, buffer = cv2.imencode(".jpg", best_thumbnail["image_cv2"])
    if not ret:
        return None

    img_base64 = base64.b64encode(buffer).decode("utf-8")

    return {
        "time_sec": best_thumbnail["time_sec"],
        "score": best_thumbnail["score"],
        "image_base64": img_base64,
    }


# =======================================
# 2. STT + ìš”ì•½ + ì œëª© ìƒì„± (Gemini 2.5 Flash)
# =======================================

def extract_audio(video_path, audio_path="temp_audio.mp3"):
    """
    ë¹„ë””ì˜¤ íŒŒì¼ì—ì„œ ì˜¤ë””ì˜¤ë¥¼ ì¶”ì¶œí•˜ì—¬ mp3ë¡œ ì €ì¥
    """
    try:
        video = AudioSegment.from_file(video_path)
        video.export(audio_path, format="mp3")
        return audio_path
    except Exception as e:
        raise RuntimeError(f"Audio extraction failed: {e}")


def analyze_video_content(video_path, api_key):
    """
    ë¹„ë””ì˜¤ â†’ ì˜¤ë””ì˜¤ ì¶”ì¶œ â†’ Gemini STT + ìš”ì•½ + ì œëª© ìƒì„±
    transcript / summary / title ë°˜í™˜
    """
    if api_key is None or api_key.strip() == "" or api_key == "YOUR_API_KEY_HERE":
        raise ValueError("ìœ íš¨í•œ Google API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤.")

    # Gemini API í‚¤ ì„¤ì •
    genai.configure(api_key=api_key)

    # 1. ì˜¤ë””ì˜¤ íŒŒì¼ ìƒì„±
    audio_file_path = extract_audio(video_path)

    # 2. Gemini íŒŒì¼ ì—…ë¡œë“œ
    try:
        audio_file = genai.upload_file(path=audio_file_path)
    except Exception as e:
        raise RuntimeError(f"Gemini audio upload failed: {e}")

    # 3. Gemini ëª¨ë¸
    model = genai.GenerativeModel("models/gemini-2.5-flash-preview-09-2025")

    # 4. í”„ë¡¬í”„íŠ¸
    prompt = """
    ì´ ì˜¤ë””ì˜¤ íŒŒì¼ì€ ê°€ì¡± ì¼ê¸°ì…ë‹ˆë‹¤. ë‹¤ìŒ ì‘ì—…ì„ ìˆ˜í–‰í•˜ì„¸ìš”:

    1. [STT]: ì˜¤ë””ì˜¤ì˜ ë‚´ìš©ì„ í•œêµ­ì–´ í…ìŠ¤íŠ¸ë¡œ ëª¨ë‘ ë°›ì•„ì“°ê¸°
    2. [ìš”ì•½]: ì¤‘ìš”í•œ ë‚´ìš©ë§Œ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½
       (ëŒ€í™”ì²´ ë§íˆ¬ ê¸ˆì§€ â€” ì‚¬ì‹¤ ê¸°ë°˜ ìš”ì•½)
    3. [ì œëª©]: ì´ ì˜ìƒì˜ ì£¼ìš” ì£¼ì œë¥¼ ë°˜ì˜í•œ ë§¤ìš° ê°„ê²°í•œ ì œëª© ìƒì„±
       (ì˜ˆ: â€œì˜¤ëŠ˜ì˜ ê°€ì¡± ì—¬í–‰â€, â€œì•„ì´ì˜ í•™êµ ìƒí™œ ì´ì•¼ê¸°â€ ê°™ì€ í˜•ì‹)

    ë°˜ë“œì‹œ JSON í˜•íƒœë¡œë§Œ ì‘ë‹µ:
    {
      "transcript": "...",
      "summary": "...",
      "title": "..."
    }
    """

    try:
        response = model.generate_content([audio_file, prompt])
        text = response.text.strip()

        # ```json ... ``` í˜•ì‹ìœ¼ë¡œ ì˜¤ëŠ” ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ ì–‘ìª½ ì •ë¦¬
        clean_text = text.lstrip("```json").rstrip("```").strip()
        results = json.loads(clean_text)

        transcript = results.get("transcript", "")
        summary = results.get("summary", "")
        title = results.get("title", "")

    except Exception as e:
        raise RuntimeError(f"Gemini ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")

    finally:
        # ë¡œì»¬ ì„ì‹œ ì˜¤ë””ì˜¤ ì‚­ì œ
        if os.path.exists(audio_file_path):
            os.remove(audio_file_path)

        # Gemini ì„œë²„ì— ì—…ë¡œë“œëœ íŒŒì¼ ì‚­ì œ
        try:
            genai.delete_file(audio_file.name)
        except Exception:
            pass

    return {
        "transcript": transcript,
        "summary": summary,
        "title": title,
    }
