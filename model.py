# model.py
"""
ì›ƒëŠ” ì–¼êµ´ ì¸ë„¤ì¼ + STT/ìš”ì•½/ì œëª© ìƒì„± í†µí•© AI ëª¨ë“ˆ

- find_best_thumbnail(video_path): Google Cloud Vision ê¸°ë°˜ ì›ƒëŠ” ì–¼êµ´ ì¸ë„¤ì¼ ì„ ì •
- analyze_video_content(video_path, api_key): Gemini 2.5 Flash ê¸°ë°˜ STT + ìš”ì•½ + ì œëª© ìƒì„±
"""

import os
import cv2
import base64
import numpy as np
from google.cloud import vision
import mediapipe as mp

# ============================================================
# 0. Vision API ì´ˆê¸°í™”
# ============================================================
if not os.environ.get("GOOGLE_APPLICATION_CREDENTIALS"):
    os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "service-account.json"

vision_client = vision.ImageAnnotatorClient()

# Mediapipe ì´ˆê¸°í™”
mp_face = mp.solutions.face_detection
mp_facedetector = mp_face.FaceDetection(model_selection=0, min_detection_confidence=0.45)


# ============================================================
# 1. Mediapipe 1ì°¨ í•„í„°ë§ (ë¹ ë¥¸ ì–¼êµ´/ì›ƒìŒ í›„ë³´ íƒì§€)
# ============================================================

def is_smile_candidate(frame):
    """
    Mediapipeë¡œ ë¹ ë¥´ê²Œ ì›ƒì„ ê°€ëŠ¥ì„± ìˆëŠ” í”„ë ˆì„ì¸ì§€ íŒë‹¨
    - ì…ì´ í¬ê²Œ ë²Œì–´ì¡ŒëŠ”ì§€
    - ì…ê¼¬ë¦¬ê°€ ì˜¬ë¼ê°”ëŠ”ì§€
    """

    results = mp_facedetector.process(frame)
    if not results.detections:
        return False  # ì–¼êµ´ ì—†ìŒ â†’ ì œê±°

    det = results.detections[0]

    # Bounding box
    box = det.location_data.relative_bounding_box
    h, w, _ = frame.shape
    x1, y1 = int(box.xmin * w), int(box.ymin * h)
    x2, y2 = x1 + int(box.width * w), y1 + int(box.height * h)

    face_roi = frame[y1:y2, x1:x2]
    if face_roi.size == 0:
        return False

    # ë‹¨ìˆœ ì…ìƒ‰ì—­(í•˜ë‹¨ 40%)ì—ì„œ ì… ë²Œì–´ì§ ì²´í¬ â†’ ë§¤ìš° ë¹ ë¦„
    roi_h = face_roi.shape[0]
    mouth_region = face_roi[int(roi_h*0.55): int(roi_h*0.85), :]

    if mouth_region.size == 0:
        return False

    # ì… ì£¼ë³€ ëŒ€ë¹„ ì¦ê°€ â†’ ì… ë²Œë ¸ì„ í™•ë¥  â†‘
    gray = cv2.cvtColor(mouth_region, cv2.COLOR_BGR2GRAY)
    variance = gray.var()  # í‘œì • ë³€í™”(ì… ëª¨ì–‘ ë³€í™”)ë¡œ varianceê°€ ì¦ê°€í•¨

    return variance > 40   # ê²½í—˜ì  threshold (ì¡°ì ˆ ê°€ëŠ¥)


# ============================================================
# 2. Vision API Batch ì–¼êµ´ ë¶„ì„ (ì •í™•í•œ ê°ì •/ì›ƒìŒ íŒë³„)
# ============================================================

LIKELIHOOD_SCORE = {
    "UNKNOWN": 0, "VERY_UNLIKELY": 0, "UNLIKELY": 1,
    "POSSIBLE": 2, "LIKELY": 4, "VERY_LIKELY": 5
}

def analyze_batch(frames):
    """
    Vision API BatchAnnotateImagesë¡œ ì—¬ëŸ¬ í”„ë ˆì„ì„ í•œë²ˆì— ì²˜ë¦¬
    """

    requests = []
    for f in frames:
        image = vision.Image(content=f["image_bytes"])
        requests.append(vision.AnnotateImageRequest(image=image, features=[
            vision.Feature(type_=vision.Feature.Type.FACE_DETECTION)
        ]))

    response = vision_client.batch_annotate_images(requests=requests)

    results = []
    for frame, res in zip(frames, response.responses):
        faces = res.face_annotations

        if not faces:
            frame["score"] = 0
            results.append(frame)
            continue

        total_score = 0

        for face in faces:
            base_quality = 0
            if LIKELIHOOD_SCORE.get(face.blurred_likelihood, 0) < 3: base_quality += 40
            if LIKELIHOOD_SCORE.get(face.under_exposed_likelihood, 0) < 3: base_quality += 20
            if abs(face.roll_angle) < 20 and abs(face.pan_angle) < 20: base_quality += 20

            joy_score = LIKELIHOOD_SCORE.get(face.joy_likelihood, 0) / 5.0 * 300

            total_score += base_quality + joy_score

        frame["score"] = total_score
        results.append(frame)

    return results


# ============================================================
# 3. í”„ë ˆì„ ì¶”ì¶œ + 1ì°¨ í•„í„°ë§
# ============================================================

def extract_candidate_frames(video_path, sec_interval=0.25):
    """
    ëª¨ë“  í”„ë ˆì„ì„ ì¶”ì¶œí•˜ì§€ë§Œ,
    Mediapipeë¡œ â€˜ì›ƒìŒ í›„ë³´â€™ë§Œ ë°˜í™˜ (80~95% ì œê±°ë¨)
    """
    cap = cv2.VideoCapture(video_path)
    fps = cap.get(cv2.CAP_PROP_FPS)
    step = int(fps * sec_interval)

    frames = []
    frame_idx = 0
    total = 0

    while True:
        ret, frame = cap.read()
        if not ret:
            break
        total += 1

        if frame_idx % step == 0:
            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

            if is_smile_candidate(rgb):
                ok, buffer = cv2.imencode(".jpg", frame)
                if ok:
                    frames.append({
                        "time_sec": frame_idx / fps,
                        "image_bytes": buffer.tobytes(),
                        "image_cv2": frame,
                    })

        frame_idx += 1

    cap.release()

    print(f"âš¡ ì „ì²´ í”„ë ˆì„: {total} â†’ í›„ë³´ í”„ë ˆì„: {len(frames)}ê°œ (ì†ë„ {total/len(frames):.1f}ë°° í–¥ìƒ ì˜ˆìƒ)")
    return frames


# ============================================================
# 4. ë©”ì¸: ìµœì¢… ì¸ë„¤ì¼ ì°¾ê¸°
# ============================================================

def find_best_thumbnail(video_path):
    # --- 1ì°¨ í•„í„°ë§ ---
    candidates = extract_candidate_frames(video_path)

    if len(candidates) == 0:
        print("ğŸ˜¢ ì›ƒëŠ” ì–¼êµ´ í›„ë³´ ì—†ìŒ. ì˜ìƒ ì¤‘ê°„ ì¸ë„¤ì¼ ë°˜í™˜")
        return None

    # ë„ˆë¬´ ë§ìœ¼ë©´ 30ì¥ë§Œ Vision APIë¡œ ë¶„ì„
    if len(candidates) > 30:
        candidates = candidates[:30]

    # --- Vision API batch ë¶„ì„ ---
    scored = analyze_batch(candidates)

    # ìµœê³  ì ìˆ˜ í”„ë ˆì„ ì„ íƒ
    scored.sort(key=lambda x: x["score"], reverse=True)
    best = scored[0]

    print(f"ğŸ‰ ìµœì¢… ì¸ë„¤ì¼ ê²°ì •! (score={best['score']:.1f}, time={best['time_sec']:.2f}s)")

    ok, buffer = cv2.imencode(".jpg", best["image_cv2"])
    img_base64 = base64.b64encode(buffer).decode("utf-8")

    return {
        "time_sec": best["time_sec"],
        "score": best["score"],
        "image_base64": img_base64,
    }


import os
import json
from pydub import AudioSegment
import google.generativeai as genai

# =======================================
# 2. ìš”ì•½ + ì œëª© ìƒì„± (Gemini 2.5 Flash)
# =======================================

def extract_audio(video_path, audio_path="temp_audio.mp3"):
    """
    ë¹„ë””ì˜¤ì—ì„œ ì˜¤ë””ì˜¤ë§Œ MP3ë¡œ ì¶”ì¶œ
    """
    try:
        video = AudioSegment.from_file(video_path)
        video.export(audio_path, format="mp3")
        return audio_path
    except Exception as e:
        raise RuntimeError(f"Audio extraction failed: {e}")


def analyze_video_content(video_path, api_key):
    """
    ë¹„ë””ì˜¤ â†’ ì˜¤ë””ì˜¤ ì¶”ì¶œ â†’ Geminië¡œ ìš”ì•½ + ì œëª© ìƒì„±
    (upload_file ì œê±° / ë°”ì´ë„ˆë¦¬ ì§ì ‘ ì…ë ¥)
    """
    if api_key is None or api_key.strip() == "":
        raise ValueError("ìœ íš¨í•œ Google API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤.")

    # Gemini API ì„¤ì •
    genai.configure(api_key=api_key)

    # 1. ì˜¤ë””ì˜¤ íŒŒì¼ ì¶”ì¶œ
    audio_file_path = extract_audio(video_path)

    try:
        # 2. ì˜¤ë””ì˜¤ ë°”ì´ë„ˆë¦¬ ì§ì ‘ ì½ê¸°
        with open(audio_file_path, "rb") as f:
            audio_bytes = f.read()

        # 3. ë¹ ë¥¸ ëª¨ë¸
        model = genai.GenerativeModel("models/gemini-2.5-flash")

        # 4. ê°„ê²°í•˜ê³  ì†ë„ ë¹ ë¥¸ í”„ë¡¬í”„íŠ¸
        prompt = """
        ì´ ì˜¤ë””ì˜¤ ë‚´ìš©ì„ í•œêµ­ì–´ë¡œ í•œ ë¬¸ì¥ ìš”ì•½í•˜ê³ ,
        ì˜ìƒì˜ ì£¼ì œë¥¼ ë°˜ì˜í•œ ê°„ê²°í•œ ì œëª©ì„ ìƒì„±í•˜ì„¸ìš”.

        ë°˜ë“œì‹œ JSON:
        {
          "summary": "...",
          "title": "..."
        }
        """

        # 5. íŒŒì¼ ì—…ë¡œë“œ ëŒ€ì‹  ë°”ì´ë„ˆë¦¬ ì§ì ‘ ì „ë‹¬
        response = model.generate_content(
            [ 
                {"mime_type": "audio/mpeg", "data": audio_bytes}, 
                prompt
            ]
        )

        clean_text = response.text.strip().lstrip("```json").rstrip("```").strip()
        results = json.loads(clean_text)

        summary = results.get("summary", "")
        title = results.get("title", "")

    except Exception as e:
        raise RuntimeError(f"Gemini ë¶„ì„ ì˜¤ë¥˜: {e}")

    finally:
        # ì„ì‹œ ì˜¤ë””ì˜¤ ì‚­ì œ
        if os.path.exists(audio_file_path):
            os.remove(audio_file_path)

    return {
        "summary": summary,
        "title": title
    }
