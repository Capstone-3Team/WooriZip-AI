"""
ì›ƒëŠ” ì–¼êµ´ ì¸ë„¤ì¼ + ìš”ì•½/ì œëª© ìƒì„±
ì„¸ë¡œ ì˜ìƒ ìë™ íšŒì „ ë³´ì • + FaceMesh + Flash Lite (ìµœì í™” ë²„ì „)
"""

import os
import cv2
import base64
import json
import numpy as np
import subprocess
from pydub import AudioSegment
from pydub.effects import speedup
from google.cloud import vision
import google.generativeai as genai
import mediapipe as mp


# ============================================================
# 0. Vision API ì´ˆê¸°í™”
# ============================================================
if not os.environ.get("GOOGLE_APPLICATION_CREDENTIALS"):
    os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "service-account.json"

vision_client = vision.ImageAnnotatorClient()


# ============================================================
# 1. ffprobeë¡œ íšŒì „ ì •ë³´ ì½ê¸°
# ============================================================
def get_rotation(video_path):
    """
    ffprobeë¡œ ì˜ìƒ metadataì—ì„œ íšŒì „ ì •ë³´ ì½ê¸°
    90 / 180 / 270 / ì—†ìœ¼ë©´ 0
    """
    try:
        cmd = [
            "ffprobe", "-v", "error", "-select_streams", "v:0",
            "-show_entries", "stream_tags=rotate",
            "-of", "default=noprint_wrappers=1:nokey=1",
            video_path
        ]
        output = subprocess.check_output(cmd).decode().strip()
        return int(output) if output else 0
    except:
        return 0


def correct_rotation(frame, rotation):
    """
    íšŒì „ metadataì— ë”°ë¼ í”„ë ˆì„ íšŒì „ ë³´ì •
    """
    if rotation == 90:
        return cv2.rotate(frame, cv2.ROTATE_90_CLOCKWISE)
    elif rotation == 180:
        return cv2.rotate(frame, cv2.ROTATE_180)
    elif rotation == 270:
        return cv2.rotate(frame, cv2.ROTATE_90_COUNTERCLOCKWISE)
    return frame


# ============================================================
# 2. FaceMesh ê¸°ë°˜ í•„í„°ë§
# ============================================================
mp_face_mesh = mp.solutions.face_mesh
mesh_detector = mp_face_mesh.FaceMesh(
    max_num_faces=1,
    refine_landmarks=True,
    min_detection_confidence=0.5,
    min_tracking_confidence=0.5
)

UPPER_LIP = 13
LOWER_LIP = 14
LEFT_MOUTH = 61
RIGHT_MOUTH = 291


def is_smile_candidate(frame):
    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    result = mesh_detector.process(rgb)

    if not result.multi_face_landmarks:
        return False

    lm = result.multi_face_landmarks[0].landmark
    h, w, _ = frame.shape

    def pos(idx):
        return np.array([lm[idx].x * w, lm[idx].y * h])

    upper = pos(UPPER_LIP)
    lower = pos(LOWER_LIP)
    left = pos(LEFT_MOUTH)
    right = pos(RIGHT_MOUTH)

    # 1) ì… ë²Œì–´ì§
    lip_distance = np.linalg.norm(upper - lower)

    # 2) ì…ê¼¬ë¦¬ ê³¡ë¥ 
    center = (upper + lower) / 2
    curvature = (center[1] - left[1]) + (center[1] - right[1])

    # ê¸°ë³¸ smile score
    smile_score = curvature * 0.6 + lip_distance * 0.4

    # threshold ê°•í™” ì œê±° â†’ ê¸°ë³¸ê°’ë§Œ
    return smile_score > 6


# ============================================================
# 3. Vision API Batch ë¶„ì„
# ============================================================
LIKELIHOOD_SCORE = {
    "UNKNOWN": 0,
    "VERY_UNLIKELY": 0,
    "UNLIKELY": 1,
    "POSSIBLE": 2,
    "LIKELY": 4,
    "VERY_LIKELY": 5,
}

def analyze_batch(frames):
    MAX_BATCH = 16
    all_results = []

    for i in range(0, len(frames), MAX_BATCH):
        chunk = frames[i:i + MAX_BATCH]

        requests = [
            vision.AnnotateImageRequest(
                image=vision.Image(content=f["image_bytes"]),
                features=[vision.Feature(type_=vision.Feature.Type.FACE_DETECTION)]
            )
            for f in chunk
        ]

        response = vision_client.batch_annotate_images(requests=requests)

        for frame, res in zip(chunk, response.responses):
            faces = res.face_annotations

            if not faces:
                frame["score"] = 0
                all_results.append(frame)
                continue

            total_score = 0
            for face in faces:
                blur_val = LIKELIHOOD_SCORE.get(face.blurred_likelihood.name, 0)
                under_val = LIKELIHOOD_SCORE.get(face.under_exposed_likelihood.name, 0)
                joy_val = LIKELIHOOD_SCORE.get(face.joy_likelihood.name, 0)

                base = 0
                if blur_val < 3: base += 40
                if under_val < 3: base += 20
                if abs(face.roll_angle) < 20 and abs(face.pan_angle) < 20:
                    base += 20

                joy_score = joy_val / 5.0 * 300
                total_score += base + joy_score

            frame["score"] = total_score
            all_results.append(frame)

    return all_results


# ============================================================
# 4. íšŒì „ ë³´ì • í¬í•¨ í›„ë³´ í”„ë ˆì„ ì¶”ì¶œ
# ============================================================
def extract_candidate_frames(video_path, sec_interval=0.35):
    rotation = get_rotation(video_path)

    cap = cv2.VideoCapture(video_path)
    fps = cap.get(cv2.CAP_PROP_FPS) or 30.0
    step = max(int(fps * sec_interval), 1)

    frames = []
    frame_idx = 0
    total = 0

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        # â­ íšŒì „ ë³´ì •
        frame = correct_rotation(frame, rotation)

        total += 1
        if frame_idx % step == 0:
            if is_smile_candidate(frame):
                ok, buffer = cv2.imencode(".jpg", frame)
                if ok:
                    frames.append({
                        "time_sec": frame_idx / fps,
                        "image_cv2": frame,
                        "image_bytes": buffer.tobytes(),
                    })

        frame_idx += 1

    cap.release()
    print(f"âš¡ ì „ì²´ {total}í”„ë ˆì„ â†’ í›„ë³´ {len(frames)}ê°œ")
    return frames


# ============================================================
# 5. ìµœì¢… ì¸ë„¤ì¼
# ============================================================
def find_best_thumbnail(video_path):
    candidates = extract_candidate_frames(video_path)

    if len(candidates) == 0:
        return None

    if len(candidates) > 12:
        candidates = candidates[:12]

    scored = analyze_batch(candidates)
    scored.sort(key=lambda x: x["score"], reverse=True)
    best = scored[0]

    ok, buffer = cv2.imencode(".jpg", best["image_cv2"])
    img_base64 = base64.b64encode(buffer).decode("utf-8")

    print(f"ğŸ‰ ìµœì¢… ì¸ë„¤ì¼ (score={best['score']:.1f}, time={best['time_sec']:.2f}s)")

    return {
        "time_sec": best["time_sec"],
        "score": best["score"],
        "image_base64": img_base64,
    }


# ============================================================
# 6. ì˜¤ë””ì˜¤ â†’ 1.2x â†’ Gemini Flash Lite
# ============================================================
def extract_audio(video_path, audio_path="temp_audio.mp3"):
    try:
        audio = AudioSegment.from_file(video_path)

        # ì˜¤ë””ì˜¤ ê¸¸ì´ ê°ì†Œ (1.2x)
        audio = speedup(audio, playback_speed=1.2, chunk_size=60, crossfade=40)
        audio.export(audio_path, format="mp3")

        return audio_path
    except Exception as e:
        raise RuntimeError(f"Audio extraction failed: {e}")


def analyze_video_content(video_path, api_key):
    if not api_key:
        raise ValueError("ìœ íš¨í•œ Google API Key í•„ìš”")

    genai.configure(api_key=api_key)

    audio_file_path = extract_audio(video_path)

    try:
        with open(audio_file_path, "rb") as f:
            audio_bytes = f.read()

        model = genai.GenerativeModel("models/gemini-2.5-flash-lite")

        prompt = """
        ì´ ì˜¤ë””ì˜¤ ë‚´ìš©ì„ í•œêµ­ì–´ë¡œ í•œ ë¬¸ì¥ ìš”ì•½í•˜ê³ ,
        ì˜ìƒì˜ ì£¼ì œë¥¼ ë°˜ì˜í•œ ê°„ê²°í•œ ì œëª©ì„ ìƒì„±í•˜ì„¸ìš”.
        ë°˜ë“œì‹œ JSON:
        {
          "summary": "...",
          "title": "..."
        }
        """

        response = model.generate_content(
            [
                {"mime_type": "audio/mpeg", "data": audio_bytes},
                prompt
            ]
        )

        clean = response.text.strip().lstrip("```json").rstrip("```").strip()
        result = json.loads(clean)

        return {
            "summary": result.get("summary", ""),
            "title": result.get("title", "")
        }

    except Exception as e:
        raise RuntimeError(f"Gemini ë¶„ì„ ì˜¤ë¥˜: {e}")

    finally:
        if os.path.exists(audio_file_path):
            os.remove(audio_file_path)
